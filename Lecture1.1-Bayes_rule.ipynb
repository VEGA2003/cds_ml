{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513d5802-b94e-4360-9d86-ed809304397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import factorial, comb\n",
    "from scipy.special import gamma as gamma_func\n",
    "from scipy.special import beta as beta_func\n",
    "from scipy.stats import gamma, beta, binom\n",
    "\n",
    "from cubature import cubature\n",
    "\n",
    "from utils import color_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c405102-efce-4eab-b5d5-50f44a51eacc",
   "metadata": {},
   "source": [
    "# Lecture 1: Probability, entropy and inference\n",
    "\n",
    "* **1.1 Bayesian inference**\n",
    "    * Probability and Bayes rule\n",
    "    * Inference with discrete random variables\n",
    "    * Inference with continuous random variables\n",
    "\n",
    "MacKay, **Chapter 2** (sections 2.1, 2.1, 2.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "310b0a89-ee16-40f3-8869-0b64a002ad67",
   "metadata": {},
   "source": [
    "# Probability: the <em>forward</em> way\n",
    "\n",
    "You can think of conditional probabilities as \"Forward probabilities\", i.e. the probabilities of possible outcomes given a probability model\n",
    "\n",
    "$$p(x|f) \\rightarrow p(\\text{outcome}|f)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8339e9a9-35ac-4440-be94-922906df5984",
   "metadata": {},
   "source": [
    "## Exercise (Mackay Ex 2.4)\n",
    "\n",
    "Urn with $B$ black balls and $W$ white balls, with total $K=W+B$. \n",
    "\n",
    "Draw a ball from the urn $N$ times, with replacement.\n",
    "\n",
    "What is the probability to draw $N_B$ black balls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84565ea3-15ff-4b1f-849c-ea54a2dea5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urn composition\n",
    "W = 8\n",
    "B = 2\n",
    "K = W + B\n",
    "\n",
    "# prob of extracting a black ball\n",
    "f = B / K\n",
    "\n",
    "# total number of draws\n",
    "Ns = [5, 400]\n",
    "\n",
    "# run experiments with N draws a bunch of times\n",
    "np.random.seed(1)\n",
    "num_experiment = 1000\n",
    "Nbs_exp = np.zeros((len(Ns), num_experiment))\n",
    "for iN, N in enumerate(Ns):\n",
    "    samples = np.random.rand(num_experiment, N) <= f\n",
    "    Nbs_exp[iN] = samples.sum(1)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "for iN, N in enumerate(Ns):\n",
    "    plt.subplot(1,2,iN+1)\n",
    "    plt.title(f\"N={Ns[iN]}\")\n",
    "    plt.hist(Nbs_exp[iN], density=False, histtype=\"bar\");\n",
    "    plt.xlabel('Nb')\n",
    "    plt.ylabel('# Nb')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8ff2b62-c098-4c83-aa7d-fdd6218dfbf5",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Define $f=B/K$ the fraction of black balls in the urn. Then \n",
    "$$\n",
    "p(N_B)=\\binom{N}{N_N}f^{N_B}(1-f)^{N-N_B}\n",
    "$$\n",
    "\n",
    "Expected value:\n",
    "$$\n",
    "\\mathbb{E} N_B=\\sum_{N_B=0}^N p(N_B)N_B=\\ldots=Nf\\qquad\n",
    "$$\n",
    "\n",
    "Variance:\n",
    "$$\n",
    "\\mathbb{V} N_B = \\sum_{N_B=0}^N p(N_B)(N_B-Nf)^2=\\ldots=Nf(1-f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbef55-f930-40c7-a367-5ab5650d8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize solution\n",
    "plt.figure(figsize=(12,4))\n",
    "for iN, N in enumerate(Ns):\n",
    "    # compute mean and std\n",
    "    mean, std = f * N, np.sqrt(f * (1 - f) * N)\n",
    "    # compute probabilities and compare to experimental frequencies\n",
    "    Nbs = np.arange(N)\n",
    "    ps = comb(N, Nbs) * f**Nbs * (1-f)**(N-Nbs)\n",
    "    plt.subplot(1,2,iN+1)\n",
    "    plt.title(\"N={:d}, Nb = {:.2g} $\\pm$ {:.2g}\".format(Ns[iN], mean, std))\n",
    "    plt.hist(Nbs_exp[iN], bins=Nbs-0.5, density=True, alpha=0.5);\n",
    "    plt.plot(Nbs, ps, '.-')\n",
    "    plt.xlabel('Nb')\n",
    "    plt.ylabel('# Nb')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c224e4bd-48c1-4e82-b1bc-8420160c7dbe",
   "metadata": {},
   "source": [
    "# Probability: the <em>inverse</em> way</center>\n",
    "\n",
    "When faced with the occurrence of an **unknown event**, we'd like to compute \"Inverse Probabilities\": given a specific outcome, what is the probability that this has been generated by a model with parameters $f$?\n",
    "\n",
    "$$\n",
    "\\text{outcome} \\rightarrow p(f|\\text{outcome}) \n",
    "$$\n",
    "\n",
    "**Solution**: use Bayes' theorem\n",
    "\n",
    "$$\n",
    "p(f|\\text{outcome}) = \\frac{p(\\text{outcome}|f) p(f)}{p(\\text{outcome})}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d5fe8ad-1623-4872-9481-80be055334a2",
   "metadata": {},
   "source": [
    "## Exercise (Mackay Ex 2.6)\n",
    "\n",
    "There are eleven urns labelled by $u=0,\\ldots,10$, each containing ten balls. Urn $u$ contains $u$ black balls and $10 − u$ white balls. Fred selects an urn $u$ at random and draws $N$ times with replacement from that urn, obtaining $N_B$ blacks and $N − N_B$ whites.\n",
    "\n",
    "Fred’s friend, Bill, looks on. If after $N = 10$ draws $N_B = 3$ blacks have been drawn, what is the probability that the urn Fred is using is urn $u$, from Bill’s point of view? (Bill doesn’t know the value of $u$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62f981-4269-48b0-baa6-ac8dd7b91586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set urn composition and compute probability for each urn\n",
    "num_urns = 11\n",
    "N = 10 # total number of draws\n",
    "\n",
    "# compute probabilities for each urn\n",
    "fus = np.arange(num_urns) / (num_urns - 1)\n",
    "\n",
    "# run a bunch of experiments\n",
    "num_experiment = 100000\n",
    "\n",
    "np.random.seed(1)\n",
    "uNbs = np.zeros((num_experiment, 2))\n",
    "for iexp in range(num_experiment):\n",
    "    u = np.random.randint(num_urns) # select an urn at random\n",
    "    samples = np.random.rand(N) <= fus[u] # draw N times from urn u\n",
    "    uNbs[iexp] = u, samples.sum() # store urn number and number of black balls\n",
    "\n",
    "bins = (np.arange(num_urns + 1) - 0.5, np.arange(N + 1 + 1) -0.5)\n",
    "joint_frequencies, xedges, yedges = np.histogram2d(uNbs[:,0], uNbs[:,1], density=True, bins=bins)\n",
    "plt.imshow(joint_frequencies);\n",
    "plt.xlabel('Nb')\n",
    "plt.ylabel('u');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6888feee-a926-4ac9-a58c-9084f2aff41f",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We treat $u$ and $N_B$ as a random variables. We'after\n",
    "\n",
    "$$p\\left(u|N_{B},N\\right)$$\n",
    "\n",
    "First of all the choice of the urn does not depend on the outcome:\n",
    "$$\n",
    "p(u|N)=p(u)=\\frac{1}{11}\n",
    "$$\n",
    "\n",
    "From the joint\n",
    "$$\n",
    "p(u,N_B|N)=p(N_B|u,N)p(u|N)\n",
    "$$\n",
    "\n",
    "we compute the posterior:\n",
    "$$\n",
    "p(u|N_B,N)=\\frac{p(N_B|u,N)p(u|N)}{p(N_B|N)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$\n",
    "p(N_B|u,N)=\\binom{N}{N_B}f_u^{N_B}(1-f_u)^{N-N_B}\\qquad f_u=\\frac{u}{10}\n",
    "$$\n",
    "\n",
    "All in all we have:\n",
    "$$\n",
    "p(N_B|N)=\\sum_{u=0}^{10} p(u)p(N_B|u,N)=\\frac{1}{11}\\binom{N}{N_B}\\sum_{u=0}^{10} f_u^{N_B}(1-f_u)^{N-N_B}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce46980d-977f-45dd-8463-bdccb793c56d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Final expression:\n",
    "\n",
    "$$\n",
    "p(u|N_B,N)=\\frac{f_u^{N_B}(1-f_u)^{N-N_B}}{\\sum_{u'=0}^{10}f_{u'}^{N_B}(1-f_{u'})^{N-N_B}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc4dcff8-b0fa-4525-9941-80ac6c179930",
   "metadata": {},
   "source": [
    "### Visualizing the solution\n",
    "\n",
    "Define function to compute conditional probability of $N_B$ given $u$ and $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3961092d-a889-4dd5-aee6-a5303f2e0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pNb_uN(Nb, u, N, fus = None):\n",
    "    # default to 11 urns\n",
    "    if fus is None:\n",
    "        fus = np.arange(11) / 10\n",
    "    # select urn\n",
    "    fu = fus[u]\n",
    "    # compute likelihood for Nb black balls in N tosses\n",
    "    p = comb(N, Nb) * fu**Nb * (1-fu)**(N-Nb)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c61883c-efeb-4c34-a54e-48f0a646b5a1",
   "metadata": {},
   "source": [
    "Compute joint probability of urn $u$ and observed black balls $N_B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13a81d-ccd2-4f61-b070-df898be2572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute prob of choosing an urn\n",
    "pu_N = 1 / num_urns\n",
    "\n",
    "# compute joint over urn and tosses\n",
    "puNb_N = np.zeros((num_urns, N + 1))\n",
    "for u in range(num_urns):\n",
    "    for Nb in range(N + 1):\n",
    "        puNb_N[u,Nb] = pu_N * pNb_uN(Nb, u, N, fus=fus)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(puNb_N);\n",
    "plt.xlabel('Nb')\n",
    "plt.ylabel('u');\n",
    "\n",
    "plt.subplot(122)\n",
    "for u in range(num_urns):\n",
    "    line = plt.plot(joint_frequencies[u], '.:', label=\"freq\" if u == 0 else None);\n",
    "    plt.plot(puNb_N[u], '.-', c=line[0].get_color(), label=\"prob\" if u == 0 else None);\n",
    "plt.legend()\n",
    "plt.xlabel('Nb')\n",
    "plt.ylabel('freq of (u,Nb), p(u,Nb)')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ceb16e-ebeb-4add-967e-d43d0cd5f954",
   "metadata": {},
   "source": [
    "Gather observation $N_B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cce6de9-668b-4fdc-80ac-d4e1f1d8e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nb = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf7586-bb62-4678-9179-d2302e31d398",
   "metadata": {},
   "source": [
    "and compute posterior urns $u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378684f-e377-4c3c-8498-ad75b75fc7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(puNb_N);\n",
    "plt.plot()\n",
    "plt.xlabel('Nb')\n",
    "plt.ylabel('u')\n",
    "# select un-normalized conditional from joint\n",
    "plt.vlines(x=Nb, ymin=0, ymax=num_urns-1, color='red')\n",
    "\n",
    "plt.subplot(132)\n",
    "# visualize un-normalized conditional from joint\n",
    "pu_NbN = puNb_N[:,Nb].copy()\n",
    "plt.plot(pu_NbN, '.-', c=\"red\");\n",
    "plt.bar(np.arange(num_urns), height=joint_frequencies[:,Nb], alpha=0.5)\n",
    "plt.xlabel('u')\n",
    "plt.ylabel('p(u,Nb|N)')\n",
    "\n",
    "plt.subplot(133)\n",
    "# normalize and plot posterior\n",
    "renorm = pu_NbN.sum()\n",
    "pu_NbN /= renorm\n",
    "plt.plot(pu_NbN, '.-');\n",
    "plt.xlabel('u')\n",
    "plt.ylabel('p(u|Nb,N)')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88911bd-2938-41bd-bfb2-31a049f5a056",
   "metadata": {},
   "source": [
    "Take a moment to compare `pu_NbN` with the table in Mackay Fig. 2.6."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e21d245-9298-47bc-bfde-9b0dfa68d872",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "Bayesian inference produces **posterior probabilities** of events: we cannot know which urn was selected, but we have a distribution over $u$ that we can use to make decision on further events.\n",
    "\n",
    "In general, given a model parametrized by $\\theta$, the procedure is:\n",
    "1. Collect data;\n",
    "2. Specify the prior over models $p(\\theta)$;\n",
    "3. Compute the likelihood of the observed data under the model with parameters $\\theta$: $p(\\text{data}|\\theta)$;\n",
    "4. Compute the posterior using Bayes' rule:\n",
    "\n",
    "$$\n",
    "p(\\theta|\\text{data})=\\frac{p(\\text{data}|\\theta)p(\\theta)}{p(\\text{data})}\\qquad p(\\text{data})=\\int d\\theta p(\\text{data}|\\theta)p(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19594c4e-92da-4b37-be39-f8a20743e2a7",
   "metadata": {},
   "source": [
    "## Exercise (Mackay Ex 2.6, continued)\n",
    "\n",
    "Fred draws another ball from the same urn. Given the previous $N$ observations, what is the probability that the ball is black?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626a34a-2c05-4fb8-952c-0d091ee7d3f2",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We (Bill) don't known $u$ but we can sum over the posterior probabilities we just obtained:\n",
    "$$\n",
    "p(\\text{black}|N_B=3, N=10)=\\sum_{u=0}^{10} p(\\text{black}|u,N_B,N)p(u|N_B,N)\n",
    "$$\n",
    "\n",
    "and since the probability $p(u|N_B,N)$ of drawing from $u$ does not depend on previous observations we get:\n",
    "\n",
    "$$\n",
    "p(\\text{black}|N_B=3, N=10)=\\sum_{u=0}^{10} f_u p(u|N_B,N)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa65d7ac-03ef-40ef-8b50-05e1c948e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(black|Nb=3, N=10) =  0.3330377815118737\n"
     ]
    }
   ],
   "source": [
    "p_new_black = (fus * pu_NbN).sum()\n",
    "print(\"p(black|Nb=3, N=10) = \", p_new_black)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96abfa88-4658-4d26-8c64-9d30c4d9ec49",
   "metadata": {},
   "source": [
    "Compare with prediction from most likely urn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67685a54-6a3b-4cc4-99af-927d2e8d2372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most likely urn:  3\n",
      "p(black|u=3) = 0.3\n"
     ]
    }
   ],
   "source": [
    "most_likely_urn = np.argmax(pu_NbN)\n",
    "print(\"most likely urn: \", most_likely_urn)\n",
    "print(f\"p(black|u={most_likely_urn}) = {fus[most_likely_urn]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0c1d294-0891-4ac0-b30b-bd2f2626962f",
   "metadata": {},
   "source": [
    "## Exercise (Mackay Example 2.7 and Exercise 2.8)\n",
    "\n",
    "A (bent) coin has an unknown probability $f$ to come up head. We toss the coin $N$ times and get $N_H$ times head.\n",
    "\n",
    "What is $f$?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fe844ce-9749-48de-aa15-a36664602820",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Again we treat $f$ as a random variable. Assume a prior over $f$: $p(f)$ is our (subjective) prior belief in the value of $f$.\n",
    "\n",
    "Given $f$ and $N$, we know the **likelihood** of the observation: \n",
    "$$\n",
    "p(N_H|f,N)=\\binom{N}{N_H}f^{N_H}(1-f)^{N-N_H}\n",
    "$$\n",
    "\n",
    "and that's how they look like for different $f$'s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49b606-aff7-42d0-942b-c77a536a218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of tosses\n",
    "N = 20\n",
    "\n",
    "fs = [0.2, 0.5, 0.9]\n",
    "\n",
    "for f in fs:\n",
    "    Nhs = np.arange(N + 1)\n",
    "    plt.plot(Nhs, binom.pmf(Nhs, N, f), '.-', label=f\"f = {f}\");\n",
    "plt.xlabel(\"$N_H$\")\n",
    "plt.ylabel(f\"p($N_H$|f,N = {N})\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f886b41-fa1b-4144-ab44-13f7367d01d1",
   "metadata": {},
   "source": [
    "The posterior reads\n",
    "$$\n",
    "p(f|N_H,N)=\\frac{p(N_H|f,N)p(f)}{p(N_H|N)}\n",
    "$$\n",
    "\n",
    "The **evidence** $p(N_H|N)$, i.e. the normalization of the posterior, must be computed from\n",
    "\n",
    "$$\n",
    "\\int_0^1 df p(N_H|f,N) p(f)= \\binom{N}{N_H} \\int_0^1 df f^{N_H}(1-f)^{N-N_H} p(f)\n",
    "$$\n",
    "\n",
    "\n",
    "We could of course do it numerically. Assuming $p(f)=1$ for simplicity we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd628da3-6ccd-4d54-a5b4-31985a858b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_Nh_N = np.zeros(N + 1)\n",
    "for Nh in range(N + 1):\n",
    "    p_Nh_N[Nh] = cubature(lambda f : binom.pmf(Nh, N, f), 1, 1, [0.], [1.])[0][0]\n",
    "\n",
    "plt.plot(p_Nh_N, '.-');\n",
    "plt.xlabel('Nh')\n",
    "plt.ylabel(f'p(Nh|N={N})');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603a5de-5cde-4016-9f33-b033cae415f8",
   "metadata": {},
   "source": [
    "* Why do you think this makes sense intuitively?\n",
    "\n",
    "* Can we do it analytically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce76e99-a6f6-45fa-8536-c8fcaed2767d",
   "metadata": {},
   "source": [
    "## <em>Intermezzo: Gamma and Beta distributions</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4861ddd-e9ab-41ef-a11a-62b7870e8c8e",
   "metadata": {},
   "source": [
    "### A quick reminder of the $\\Gamma$ function\n",
    "$$\n",
    "\\Gamma\\left(z\\right)=\\int_{0}^{\\infty}t^{z-1}e^{-t}dt\\qquad\\Re\\left(z\\right)>0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma\\left(z+1\\right)=z\\Gamma\\left(z\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma\\left(n\\right) = \\left(n-1\\right)!\\qquad n\\in\\mathbb{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312dda58-40cf-4a41-9642-9c1f2ab0d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxx = 10\n",
    "xs = np.arange(1, maxx)\n",
    "plt.plot(xs, gamma_func(xs), label=\"Γ(x)\");\n",
    "plt.plot(xs, factorial(xs-1), 'o', label=\"(x - 1)!\")\n",
    "plt.legend();\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9166d6-63f3-47a1-b130-6eee73392cfe",
   "metadata": {},
   "source": [
    "### $\\Gamma$ distribution and its cumulants\n",
    "\n",
    "$$\n",
    "p\\left(x\\right)=\\frac{b^{a}}{\\Gamma\\left(a\\right)}x^{a-1}e^{-bx}\\qquad x>0\n",
    "$$"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcb9dd25-704c-44e7-aad8-244f33675807",
   "metadata": {},
   "source": [
    "# define parameters for gamma distribution\n",
    "a = 1.2 # called shape\n",
    "b = 0.4 # called rate\n",
    "\n",
    "xs_gamma = np.linspace(0, 20, 100)\n",
    "plt.plot(xs_gamma, gamma.pdf(xs_gamma, a, scale=1/b), label=\"Gamma\");\n",
    "fgamma = xs_gamma**(a-1) * np.exp(-b * xs_gamma) * b**a / gamma_func(a)\n",
    "# plt.plot(xs_gamma, fgamma, ':', label=\"just a check\"); # check math is not failing you today\n",
    "\n",
    "plt.vlines(x=a/b, ymin=0, ymax=gamma.pdf(a/b, a, scale=1/b), color='gray', ls=':', label=\"mean\");\n",
    "plt.vlines(x=(a-1)/b, ymin=0, ymax=gamma.pdf((a-1)/b, a, scale=1/b), color='gray', ls='--', label=\"mode\");\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3a0693d-7c0f-4251-961c-d2f55f9e76cb",
   "metadata": {},
   "source": [
    "### The Beta distribution\n",
    "\n",
    "The Beta distribution is a probability density over a continuous random variable $x\\in [0,1]$ defined by two shape parameters $a,b>0$. \n",
    "$$\n",
    "\\text{Beta}(x|a,b)= \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)}\n",
    "x^{a-1}(1-x)^{b-1}\\qquad 0\\leq x \\leq 1\n",
    "$$\n",
    "\n",
    "The normalization comes from the formula\n",
    "$$\n",
    "\\int_0^1 dx x^{a-1}(1-x)^{b-1}=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n",
    "$$\n",
    "\n",
    "Mean and variance \n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E} x &=& \\frac{a}{a+b}\\\\ \n",
    "\\mathbb{V} x & = & \\frac{ab}{(a+b)^2 (a+b+1)}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911eaa8-5ee2-4138-b14d-5911374af610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a bunch of shape parameters\n",
    "pars = [(0.1, 0.1),\n",
    "        (1., 1.),\n",
    "        (2., 3.),\n",
    "        (8., 4.)]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "xs_beta = np.linspace(1e-3, 1-1e-3, 100) # stay just a bit away from boundaries to avoid singularities in numerical computations\n",
    "for i, (a,b) in enumerate(pars):\n",
    "    plt.subplot(1,len(pars), i+1)\n",
    "    plt.plot(xs_beta, beta.pdf(xs_beta, a, b), label=f\"a={a}, b={b}\");\n",
    "    norm_beta = gamma_func(a) * gamma_func(b) / gamma_func(a+b)\n",
    "    fbeta = xs_beta**(a-1) * (1.-xs_beta)**(b-1) / norm_beta\n",
    "    # plt.plot(xs_beta, fbeta, ':', label=\"just a check\"); # check math is not failing you today\n",
    "    mean = a/(a+b)\n",
    "    plt.vlines(x=mean, ymin=0, ymax=beta.pdf(mean, a, b), color='gray', ls=':', label=\"mean\");\n",
    "    plt.legend();\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9145b35f-7c3c-499d-b0a2-d1ba331179fb",
   "metadata": {},
   "source": [
    "### Analytical solution\n",
    "\n",
    "The posterior\n",
    "\\begin{align}\n",
    "p(f|N_H,N)=&\\frac{ p(N_H|f,N)p(f)}{p(N_H|N)} =\\frac{\\binom{N}{N_H}}{p(N_H|N)} f^{N_H}(1-f)^{N-N_H}\\\\\n",
    "&= \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} f^{a-1}(1-f)^{b-1}\n",
    "\\end{align}\n",
    "with $N_H=a-1, N-N_H=b-1$ and $p(f)=1$.\n",
    "\n",
    "From this we can infer that\n",
    "$$\n",
    "p(N_H|N)=\\binom{N}{N_H} \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}=\\frac{N!}{N_H!(N-N_H)!} \\frac{N_H! (N-N_H)!}{(N+1)!} =\\frac{1}{N+1}\n",
    "$$\n",
    "where we used $\\Gamma(x)=(x-1)!$ when $x$ is integer. \n",
    "\n",
    "Since $p(N_H|N)=\\int df p(N_H|f,N)p(f)$ we conclude that **when integrating over all models, the probability of all outcomes are equally likely**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacde35-cc20-45d3-ac34-ce05524dab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and indeed\n",
    "plt.hlines(y=1/(N+1), xmin=0, xmax=N, ls=':', color=\"gray\", label=\"theory\")\n",
    "plt.plot(p_Nh_N, '.-', c=color_cycle[0], alpha=0.5, label=\"numeric\");\n",
    "plt.xlabel('Nh')\n",
    "plt.ylabel(f'p(Nh|N={N})')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3842b265-3d15-476b-bbf8-462014a98e7e",
   "metadata": {},
   "source": [
    "## Exercise: the bent coin - reprise\n",
    "\n",
    "What is the probability to observe another 'head' ($H$) after throwing the coin $N$ times and having observed $N_H$ times 'head'?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d4209fa-ab3f-42c5-908a-e000839b3699",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We don't known $f$ but we can integrate over it and weight each $f$ by its posterior value:\n",
    "\\begin{align}\n",
    "p(H|N_H,N)&=\\int df p(H|f) p(f|N_H,N)\\\\\n",
    "&=\\int df f \\text{Beta}(f|a=N_H+1,b=N-N_H+1) = \\frac{a}{a+b} = \\frac{N_H+1}{N+2}\n",
    "\\end{align}\n",
    "\n",
    "Suppose $N=N_H=1$.\n",
    "\n",
    "Naive answer is $f=1$ and thus $p(H|f)=1$.\n",
    "\n",
    "Bayesian answer is $p(f|N_H,N)=(N+1)f^1 (1-f)^0 = 2f$ and $p(H|N_H,N)=\\frac{N_H+1}{N+2}=\\frac{2}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ecb90-ecb8-4ab9-9df9-bb0cc049df72",
   "metadata": {},
   "source": [
    "# <center> Assignments </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3e31f-6c43-4178-9008-daf3a162f02b",
   "metadata": {},
   "source": [
    "#### Ex 1.1 (Mackay Ex 2.10)\n",
    "\n",
    "Urn A contains three balls: one black, and two white; urn B contains three balls: two black, and one white. One of the urns is selected at random and one ball is drawn. The ball is black. What is the probability that the selected urn is urn A?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4431371-7a99-457d-b81f-218a76262f60",
   "metadata": {},
   "source": [
    "#### Ex 1.2\n",
    "Consider $11$ urns $u_{i}$, $i=0,...,10$, each with $10$ balls.\n",
    "Urn $u_{i}$ has $i$ black balls and $10-i$ white balls. Select\n",
    "one urn at random, and draw $N$ times with replacement from that\n",
    "urn. Suppose that the outcome after $N=10$ draws is that the number\n",
    "of black balls that have been drawn is even. What is the probability\n",
    "that urn $u_{i}$ was selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96967959-3370-4e64-834d-e1f5e5a9a6e5",
   "metadata": {},
   "source": [
    "#### Ex 1.3\n",
    "\n",
    "Suppose we are told that data are drawn from an oracle that is a Gaussian\n",
    "distribution with $\\sigma=1$ and we believe that $\\mu$ can have\n",
    "any value with equal probability.\n",
    "* The oracle produces one data point with value $x$. What can we infer\n",
    "about $\\mu$?\n",
    "* The oracle produces a set of $N$ data points with values $x_{i},...,x_{N}$.\n",
    "What can we infer about $\\mu$ ? Hint: use the fact that the data\n",
    "points are produced independently. What is the probability that the\n",
    "data set is drawn from the oracle? Show that the result is Gaussian\n",
    "in $\\mu$ with mean $\\bar{x}=\\sum_{i=1}^{N}x_{i}$ and variance $\\frac{1}{N}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
