{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc873ef3-9838-4398-8be7-c1485f0342ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import poisson, bernoulli, multivariate_normal, entropy\n",
    "\n",
    "from utils import color_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15fda3-cc66-455c-bbf4-c1ecd37e0908",
   "metadata": {},
   "source": [
    "# Lecture 1: Probability, entropy and inference\n",
    "\n",
    "* **1.2 Entropy and variational approximation**\n",
    "    * Entropy of distributions\n",
    "    * Maximum entropy\n",
    "    * Variational approximation\n",
    "\n",
    "MacKay, **Chapter 2** (sections 2.4, 2.5, 2.6, 2.7).\n",
    "\n",
    "_Recommended reading_:\n",
    "* MacKay, **Chapter 1**\n",
    "* [Wikipedia entry](https://en.wikipedia.org/wiki/Exponential_family) for exponential families\n",
    "* [These notes](notes/exponential_families.pdf) from John Duchi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3432467-5eb2-4a64-84e5-fa065fd25855",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# What is information\n",
    "\n",
    "**Information** is a measure of the _degree of surprise_ that a certain value gives us **given that we know the distribution**.\n",
    "\n",
    "Unlikely events are informative, likely events less â†’ information decreases with the probability of the event.\n",
    "\n",
    "Given $p(x)=\\delta(x)$ (no uncertainty), observing $x$ gives us no additional information. \n",
    "\n",
    "Let us denote $h(x)$ the information of $x$. Then if $x,y$ are independent, we want information to be additive $h(x,y)=h(x)+h(y)$. Since $p(x,y)=p(x)p(y)$ we see that\n",
    "$$\n",
    "h(x)=-\\log_2 p(x)\n",
    "$$\n",
    "is a good candidate to quantify the information in $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1802bef6-b0bd-4163-a412-4215b0cc5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the candidate information/suprise function\n",
    "ps = np.linspace(1e-10, 1, 500)\n",
    "plt.plot(ps, -np.log2(ps));\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('$log_2(1/p)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644738f-1cb9-43d5-b789-0c3aac2cc8c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The **expected information** is\n",
    "$$\n",
    "H[x]:= -\\sum_x p(x)\\log_2 p(x)\n",
    "$$\n",
    "is the **entropy** of the distribution $p$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "079971cb-6a82-49e6-a1eb-33278b6cb4c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Some examples\n",
    "\n",
    "When $p$ is sharply peaked ($p(x_1)=1, p(x_2)=\\ldots=p(x_M)=0$) then the\n",
    "entropy is\n",
    "$$\n",
    "H[x]=-1\\log 1 - (M-1) 0 \\log 0 = 0\n",
    "$$\n",
    "When $p$ is flat ($p(x_i)=1/M$) the entropy is maximal\n",
    "$$\n",
    "H[x]=-M \\frac{1}{M}\\log \\frac{1}{M}=\\log M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f77e1-c210-43dc-a1a1-878c5c186660",
   "metadata": {},
   "source": [
    "Let's look at how the entropy of a Poisson distribution changes with its rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce3868-a85a-4053-b214-1bc02b6983b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate probabilities from Poisson distributions with different rates\n",
    "ks = np.arange(100)\n",
    "bins = np.arange(-0.5,100,1)\n",
    "\n",
    "num_samples = 5000\n",
    "mus = [5, 20, 40, 60]\n",
    "Hs = np.zeros(len(mus))\n",
    "Hs_scipy = np.zeros(len(mus))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "for imu, mu in enumerate(mus):\n",
    "    # compute probs\n",
    "    ps = poisson.pmf(ks, mu=mu)\n",
    "    \n",
    "    # compute entropy\n",
    "    H = -(ps * np.log(ps)).sum() # do it yourself\n",
    "    H_scipy = entropy(ps) # be lazy and let scipy do it for you\n",
    "    Hs[imu] = H\n",
    "    Hs_scipy[imu] = H_scipy\n",
    "    \n",
    "    # extract samples and plot distributions\n",
    "    samples = poisson.rvs(mu, size=num_samples);\n",
    "    plt.plot(ps, '-', c=color_cycle[imu], ms=3);\n",
    "    plt.hist(samples, bins=bins, density=True, color=color_cycle[imu], alpha=0.5, label=\"$\\mu$={:d}, H = {:.3g}\".format(mu, H));\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('p(k)')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(mus, Hs, '.-')\n",
    "plt.plot(mus, Hs_scipy, '.:') # check calculation against scipy\n",
    "plt.xlabel(\"$\\mu$\")\n",
    "plt.ylabel(\"H\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35a4884f-1c02-425d-92be-0de007a830b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Entropy and information transmission\n",
    "\n",
    "Suppose we want to send symbols $x$ over a **channel**.\n",
    "\n",
    "Symbols are assumed to be randomly generated by a distribution $p(x)$: common symbols (large $p(x)$) are sent very often, other symbols (small $p(x)$) are rarely sent.\n",
    "\n",
    "We want to encode each symbol as a binary string: we want to use short strings for common symbols and longer strings for rare symbols.\n",
    "\n",
    "The question is: how to do this optimally? \n",
    "\n",
    "#### <center>Noiseless coding theorem </center>\n",
    "Entropy is a lower bound on the average number of bits per symbol (Shannon 1948)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f91067ce-a92f-45d3-9279-fc9e4db737e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Example 1\n",
    "\n",
    "$x$ can have 8 values with equal probability, then $H(x)=-8\\times\n",
    "\\frac{1}{8}\\log \\frac{1}{8}=3$. We say that the distribution carries 3 bits of information. \n",
    "\n",
    "By Shannon's theorem, we need at least 3 bits per symbol. In fact, we can use the normal binary representation of $x$ to use exactly 3 bits per symbol.:\n",
    "\n",
    "$x$ |  $-\\log p(x)$ | code |\n",
    "| -------- | ------- | ------- |\n",
    "0\t|\t3\t| 000 |\n",
    "1\t|\t3\t| 001 |\n",
    "2\t|\t3\t| 010 |\n",
    "3\t|\t3\t| 011 |\n",
    "4\t|\t3\t| 100 |\n",
    "5\t|\t3\t| 101 |\n",
    "6\t|\t3\t| 110 |\n",
    "7\t|\t3\t| 111 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4efcb88c-1541-4664-8de1-fc56da7b8810",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Example 2\n",
    "\n",
    "$x$ can have 8 values with probabilities \n",
    "$(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\frac{1}{16},\\frac{1}{64},\\frac{1}{64},\\frac{1}{64},\\frac{1}{64})$\n",
    "\n",
    "Then \n",
    "$$\n",
    "H(x)=-\\frac{1}{2}\\log\\frac{1}{2}-\\frac{1}{4}\\log\\frac{1}{4}-\\frac{1}{8}\\log\\frac{1}{8}-\\frac{1}{16}\\log\\frac{1}{16}-\\frac{4}{64}\\log\\frac{1}{64}=2\n",
    "\\mathrm{bits}\n",
    "$$\n",
    "\n",
    "By Shannon's theorem we need on average at least two bits per symbol. The naive code as before would use 3 bits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98d64a7c-adbe-47bb-b624-51099ba34854",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Can we construct a code that saturates the Shannon bound and only uses on average 2 bits per symbol? \n",
    "\n",
    "The idea is to encode each symbol $x$ with a string with length equal to its information $-\\log p(x)$. \n",
    "Thus, for instance:\n",
    "\n",
    "$x$ |  $-\\log p(x)$ | code |\n",
    "| -------- | ------- | ------- |\n",
    "0\t|\t1\t|  0 |\n",
    "1\t|\t2\t| 10 |\n",
    "2\t|\t3\t| 110 |\n",
    "3\t|\t4\t| 1110 |\n",
    "4\t|\t6\t| 111100 |\n",
    "5\t|\t6\t| 111101 |\n",
    "6\t|\t6\t| 111110 |\n",
    "7\t|\t6\t| 111111 |\n",
    "\n",
    "Then by construction:\n",
    "$$\n",
    "\\mathrm{Average\\;code\n",
    "length}=\\sum_x p(x) \\text{length}(x) = -\\sum_x p(x) \\log_2 p(x) = H\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e72411c4-e7b1-46b3-a8d5-8d5561df6bd3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Maximum Entropy and Exponential Families\n",
    "\n",
    "The **maximum entropy** approach allows us to construct distribution $p$ to describe data that make the least amount of assumptions on the data itself, apart from including some _statistics_ from the data.\n",
    "\n",
    "Denote the data as $D=(x_1,\\ldots,x_N)$. From the data, we can compute certain <em>statistics</em>\n",
    "$$\n",
    "S_a=\\frac{1}{N}\\sum_{i=1}^N \\phi_a(x_i)\n",
    "$$\n",
    "that are averages over the data. \n",
    "\n",
    "Useful statistics might be the mean and variance of the data\n",
    "$$\n",
    "\\bar{x}=\\frac{1}{N}\\sum_{i=1}^N x_i \\qquad \\sigma^2_x=\\frac{1}{N}\\sum_{i=1}^N \\left(x_i -\\bar{x}\\right)^2=\\frac{1}{N}\\sum_i x_i^2 -\\bar{x}^2\n",
    "$$\n",
    "$$\n",
    "S_1=\\bar{x}\\qquad \\phi_1(x)=x\\qquad S_2=\\sigma^2_x+\\bar{x}^2\\qquad \\phi_2(x)=x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a6e2b-a9ad-46b3-9df8-01c6a0b1f4cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We want to find a distribution $p$ that reproduces these statistics correctly. That is to say, $p$ should be such that\n",
    "$$\n",
    "\\mathbb{E} \\phi_a =\\sum_x p(x) \\phi_a(x) = S_a\\qquad a=1,2,\\ldots\n",
    "$$\n",
    "but in general this will not determine $p$ uniquely. \n",
    "\n",
    "The maximum entropy solution is the **least informative** distribution consistent with the constraints. Thus, we wish to find \n",
    "$$\n",
    "\\min_{p(x)\\;\\text{s.t.}\\;\\mathbb{E} \\phi_a = S_a} \\int dx p(x) \\log p(x)\n",
    "$$\n",
    "where $a=0$ is the normalization constraint $\\sum_x p(x) =1$.\n",
    "\n",
    "This is a constrained optimization problem, that we can solve with **Lagrange multipliers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3815d2-0a16-4434-a0e8-a4d2139a769d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <em>Intermezzo: Lagrange multipliers</em>\n",
    "\n",
    "Minimize $f(x)$ with respect to $x$ under constraints $g_a(x) = 0$, $a=1,...,M$:\n",
    "$$\n",
    "\\min_{x\\;\\text{s.t.}\\;g_1(x)=0,\\ldots,g_M(x)=0} f(x)\n",
    "$$\n",
    "\n",
    "Define the **Lagrangian**,\n",
    "$$\n",
    "L\\left(x,\\lambda\\right) = f(x) +\\sum_{a=1}^M \\lambda_a g_a(x)\n",
    "$$\n",
    "$\\lambda_a$ is called a Lagrange multiplier.\n",
    "\n",
    "Find stationarity condition of $L\\left(x,\\lambda\\right)$ w.r.t. $x$ and $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b456d8-c478-4e35-85e6-8946bfecc132",
   "metadata": {},
   "source": [
    "More generally:\n",
    "$$\n",
    "\\min_{x\\;\\text{s.t.}\\;g_1(x)=0,\\ldots,g_M(x)=0} f(x)\\qquad\\rightarrow \\qquad \\min_x \\max_{\\lambda_1,\\ldots,\\lambda_M} f(x) +\\sum_{a=1}^M \\lambda_a g_a(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59c309-efad-4da5-a2e8-40223ffd998a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### A concrete example:\n",
    "\n",
    "Minimize\n",
    "\n",
    "$f(x_1,x_2) =  x_1^2 + x_2^2$\n",
    "\n",
    "subject to\n",
    "\n",
    "$g(x_1,x_2)  =  x_1 + x_2 -1=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb122fa-3cf6-47ee-a861-c3c96f2cf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute function values in the plane\n",
    "f = lambda x, y : x**2 + y**2\n",
    "l, r, dx = -3, 3, 0.1\n",
    "x = np.arange(l,3,0.1)\n",
    "X, Y = np.meshgrid(x, x)\n",
    "F = f(X,Y)\n",
    "\n",
    "# parametrize line\n",
    "x_line = np.arange(-2, 3, 0.01)\n",
    "y_line = -x_line + 1\n",
    "\n",
    "# plot countours and constraints\n",
    "plt.imshow(F, alpha=0.5, extent=[-3,3,-3,3], cmap=\"coolwarm\");\n",
    "plt.contour(X, Y, F)\n",
    "plt.hlines(y=0, xmin=-3, xmax=3, ls=\":\", color=\"gray\")\n",
    "plt.vlines(x=0, ymin=-3, ymax=3, ls=\":\", color=\"gray\")\n",
    "plt.scatter(x_line, y_line, c=\"black\", edgecolors='none', s=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d496a-6463-4fe2-aa9b-d00da30ac538",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Lagrangian:\n",
    "\n",
    "$L(x_1,x_2,\\lambda)  =   x_1^2 +x_2^2 + \\lambda ( x_1 + x_2 -1)$\n",
    "\n",
    "1. Maximize $L$ w.r.t. $x_{1,2}$ gives\n",
    "$x_1(\\lambda)  = x_2(\\lambda)=-\\frac{1}{2}\\lambda$.\n",
    "\n",
    "2. Plug into constraint: $\n",
    "x_1(\\lambda) + x_2(\\lambda) - 1$ yields $\\lambda^* =-1$.\n",
    "\n",
    "\n",
    "The solution is $x_1^*=x_1(\\lambda^*)=\\frac{1}{2}$ and $x_1^*=x_1(\\lambda^*)=\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a53ba-0c4a-4e44-9700-8c37c01ef8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate function on the line x + y = 1\n",
    "f_line = f(x_line, y_line)\n",
    "\n",
    "# plot solution in 2d\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(F, alpha=0.5, extent=[-3,3,-3,3], cmap=\"coolwarm\");\n",
    "plt.contour(X, Y, F)\n",
    "plt.hlines(y=0, xmin=-3, xmax=3, ls=\":\", color=\"gray\")\n",
    "plt.vlines(x=0, ymin=-3, ymax=3, ls=\":\", color=\"gray\")\n",
    "plt.scatter(x_line, y_line, c=f_line / f_line.max(), edgecolors='none', cmap=\"coolwarm\", s=3)\n",
    "plt.plot(0.5, f(0.5, -0.5 + 1.), 'o', c=\"black\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# plot value over the line\n",
    "plt.subplot(122)\n",
    "plt.plot(0.5, f(0.5, -0.5 + 1.), 'o', c=\"black\")\n",
    "plt.scatter(x_line, f_line, c=f_line / f_line.max(), edgecolors='none', cmap=\"coolwarm\", s=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x,y)')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc38ca2-38c0-44e2-be21-4971514a3831",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Solving for Maximum Entropy distribution through Lagrange Multipliers\n",
    "\n",
    "The Lagrangian for the maximum entropy problem takes the form\n",
    "$$\n",
    "L(p,\\lambda) = \\sum_x p(x)\\log p(x) + \\sum_{a=0}^M \\lambda_a \\left(\\sum_x p(x) \\phi_a(x)-S_a\\right)\n",
    "$$\n",
    "Stationarity $L$ w.r.t. $p(x)$ yields\n",
    "$$\n",
    "\\partial_{p\\left(x\\right)}L\\left(p,\\lambda \\right)=0\\implies\\log p\\left(x\\right)+1+\\lambda_{0}+\\sum_{a=1}^{M}\\lambda_{a}\\phi_{a}\\left(x\\right)=0\n",
    "$$\n",
    "from which\n",
    "$$p=e^{-1-\\lambda_{0}}\\exp\\left(-\\sum_{a=1}^{M}\\lambda_{a}\\phi_{a}\\left(x\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc3c474-10ab-4881-8fba-a45fbb7e7b3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The factor in front is fixed by the normalization condition $\\sum_x p(x)=1$. Thus calling $A=\\lambda_0+1$ we have:\n",
    "$$\n",
    "p(x)=\\exp\\left(-\\sum_{a=1}^M \\lambda_a \\phi_a(x)-A \\right)\\qquad A=\\log \\sum_{x} \\exp\\left(-\\sum_{a=1}^M \\lambda_a \\phi_a(x)\\right)\n",
    "$$\n",
    "This is called the **exponential family** model and $A$ is the **log partition sum**.\n",
    "\n",
    "The other $\\lambda_a$ are determined implicitly by the equations\n",
    "$$\\sum_x p(x) \\phi_a(x)=S_a$$\n",
    "that impose that _expectation_ of the functions $\\phi_a$ over the model distribution $p$ are the same as _the empirical average_ $S_a$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54231a43-963f-4ded-ac77-92ee05c02949",
   "metadata": {},
   "source": [
    "### Examples of exponential family distributions: Bernoulli\n",
    "\n",
    "$x=0,1$ is a binary variable and $p(x)=f^x(1-f)^{1-x}$. We can write\n",
    "$$\n",
    "p(x)=\\exp\\left(x \\log \\frac{f}{1-f} +\\log (1-f)\\right)\n",
    "$$\n",
    "Thus $\\phi_1(x)=x,\\lambda_1=-\\log \\frac{f}{1-f}, A=-\\log(1-f)$.\n",
    "\n",
    "It is the maximum entropy distribution on a binary variable for a given mean value. Note that in this case the mean determines the distribution completely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd357a-51ba-49e5-b84c-302fd15263ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0.7\n",
    "num_samples = 5000\n",
    "\n",
    "samples = bernoulli.rvs(f, size=num_samples)\n",
    "\n",
    "plt.hist(samples, bins=[-0.5,0.5,1.5], density=True, alpha=0.5, label=\"frequency\")\n",
    "plt.plot([0,1], [1-f,f], 'o-', label=\"p\")\n",
    "plt.xticks([0,1])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p')\n",
    "plt.title(f'Bernoulli with f={f}')\n",
    "plt.legend();\n",
    "\n",
    "print(\"average:\", samples.mean())\n",
    "print(\"var\", samples.var())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4fe2fb3-f664-4e75-9c94-d13b3b024ff5",
   "metadata": {},
   "source": [
    "### Examples of exponential family distributions: Gaussian\n",
    "\n",
    "$$\n",
    "p(x)\\propto \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\propto \\exp\\left(\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}x^2\\right)=\\exp\\left(-\\lambda_1 \\phi_1(x) -\\lambda_2\\phi_2(x)\\right)\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\phi_1(x)=x,\\phi_2(x)=x^2, \\lambda_1=-\\frac{\\mu}{\\sigma^2}, \\lambda_2 =\\frac{1}{2\\sigma^2}\n",
    "$$\n",
    "It is the maximum entropy distribution on a continuous variable with given mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce5116-120d-4040-89f6-fc671fd75ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([1,2])\n",
    "cov = np.array([[1., 0.4],\n",
    "                [0.4, 0.5]])\n",
    "num_samples = 100000\n",
    "\n",
    "gaussian = multivariate_normal(mean=mean, cov=cov)\n",
    "detcov = np.linalg.det(cov)\n",
    "\n",
    "dx = 0.05\n",
    "lim_std = 2\n",
    "lims_x = np.array([mean[0] - lim_std, mean[0] + lim_std])\n",
    "lims_y = np.array([mean[1] - lim_std, mean[1] + lim_std])\n",
    "xs = np.arange(lims_x[0], lims_x[1], dx)\n",
    "ys = np.arange(lims_y[0], lims_y[1], dx)\n",
    "\n",
    "# compute marginals\n",
    "pxs = np.exp(-0.5*(xs-mean[0])**2/cov[0,0])/np.sqrt(2*np.pi*cov[0,0])\n",
    "pys = np.exp(-0.5*(ys-mean[1])**2/cov[1,1])/np.sqrt(2*np.pi*cov[1,1])\n",
    "\n",
    "# set edges for proper histogramming\n",
    "xedges = np.arange(lims_x[0], lims_x[-1], dx)\n",
    "yedges = np.arange(lims_y[0], lims_y[-1], dx)\n",
    "xbins = 0.5 * (xedges[1:] + xedges[:-1])\n",
    "ybins = 0.5 * (yedges[1:] + yedges[:-1])\n",
    "xv, yv = np.meshgrid(xbins, ybins)\n",
    "x_stacked = np.dstack([xv, yv])\n",
    "\n",
    "# compute density\n",
    "pbins_true = gaussian.pdf(x_stacked)\n",
    "\n",
    "# sample from joint distribution\n",
    "samples = gaussian.rvs(size=num_samples)\n",
    "\n",
    "\n",
    "# plot stuff\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# plot histogram and joint\n",
    "plt.subplot(121)\n",
    "pbins, _, _ = np.histogram2d(samples[:,0], samples[:,1], bins=[xedges, yedges], density=True)\n",
    "plt.imshow(pbins.T, origin=\"lower\", extent=[xbins[0], xbins[-1],\n",
    "                                            ybins[0], ybins[-1]], alpha=0.7, cmap=\"coolwarm\", aspect=\"equal\");\n",
    "plt.contour(xv, yv, pbins_true, levels=3);\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# plot marginals\n",
    "plt.subplot(122)\n",
    "labels = [\"x\", \"y\"]\n",
    "for i in range(2):\n",
    "    plt.hist(samples[:,i], bins=50, density=True, alpha=0.5, color=color_cycle[i], label=labels[i]);\n",
    "    plt.plot(xs if i==0 else ys, pxs if i==0 else pys, color=color_cycle[i]);\n",
    "plt.legend();\n",
    "plt.xlabel('x/y')\n",
    "plt.ylabel('p(x), p(y)');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "309e94d7-5ace-4651-88f7-2eefa26ad479",
   "metadata": {},
   "source": [
    "## Variational approximation\n",
    "\n",
    "How do we approximate complex high-dimensional distribution with simpler ones that are easier to handle?\n",
    "\n",
    "For a given complex distribution $p(x)$, one way to approximate a distribution is to consider a parametrized set of distributions $q(x|\\theta)$ and to find \n",
    "$$\n",
    "\\theta =\\text{argmin}_\\theta d(q,p)\n",
    "$$\n",
    "\n",
    "with $d(q,p)$ some notion of **distance between distributions**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4ea6f9a-cf9e-43fa-b3cb-28d42f32fa10",
   "metadata": {},
   "source": [
    "## KL divergence\n",
    "\n",
    "We define the **Relative entropy** or **Kullback-Leibler (KL) divergence** as:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(p||q) = \\sum_i p_i \\log \\left(\\frac{ p_i} {q_i} \\right)\n",
    "$$\n",
    "\n",
    "and for continuous variables:\n",
    "$$ \\mathrm{KL}(p||q) =  - \\int p(\\mathbf{x})\\ln \\left( \\frac{ q(\\mathbf{x})}{p(\\mathbf{x})}  \\right) d\\mathbf{x}\n",
    "$$\n",
    "\n",
    "#### Properties:\n",
    "\n",
    "- $\\mathrm{KL}(p||q) \\neq \\mathrm{KL}(q||p) $ (not a distance)\n",
    "- $\\mathrm{KL}(p||q) \\geq 0$, $\\mathrm{KL}(p||q) = 0 \\Leftrightarrow p = q$ (proof through Jensen's inequality)\n",
    "- We need that when $q_i=0$ also $p_i=0$, otherwise $p_i \\log \\left(\\frac{ p_i} {q_i} \\right)$ is not defined. (The reverse is no problem: when $p_i=0$ and $q_i\\ne 0$ we have $p_i \\log \\left(\\frac{ p_i} {q_i} \\right)=0$)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c2134b2-6d8d-4c35-ba8e-97fccc2737ca",
   "metadata": {},
   "source": [
    "### _Intermezzo 1: Convex functions_\n",
    "\n",
    "Definition of a convex function:\n",
    "\n",
    "$$\\text{$f$ is convex} \\iff f(\\lambda a + (1-\\lambda) b) \\leq \\lambda f(a) + (1- \\lambda)  f(b) \\qquad \\forall \\lambda \\in [0,1], \\quad \\forall a,b$$\n",
    "\n",
    "- Examples: $f(x) = ax + b$, $f(x) = x^2$, $f(x) = -\\ln(x)$ and $f(x) = x \\ln (x)$\n",
    "- Convex: $\\cup$ shaped.  Concave: $\\cap$ shaped.\n",
    "- Convex $\\Leftarrow$ second derivative non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30b3ba-062e-490f-9e2a-7f41e2ab2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a convex (and not convex!) functions\n",
    "\n",
    "xs = np.linspace(0.1, 20, 100)\n",
    "\n",
    "functions = [lambda x : -np.log(x) + 5,\n",
    "             lambda x : x * np.log(x) + 5,\n",
    "             lambda x : x**(3/2) + 5,\n",
    "             lambda x : x**(2/3) + 5]\n",
    "\n",
    "formulas = [\"-log(x) + 5\",\n",
    "            \"x log(x) + 5\",\n",
    "            r\"$x^{\\frac{3}{2}} + 5$\",\n",
    "            r\"$x^{\\frac{2}{3}} + 5$\"]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "cols = [\"red\", \"black\"]\n",
    "for ifunc in range(4):\n",
    "    f = functions[ifunc]\n",
    "    formula = formulas[ifunc]\n",
    "    fs = f(xs)\n",
    "    \n",
    "    a, b = 1, 10\n",
    "    fa, fb = f(a), f(b)\n",
    "    ts = np.arange(0, 1.05, 0.05)\n",
    "    chord_x = a * ts + (1 - ts) * b\n",
    "    chord_y = fa * ts + (1 - ts) * fb\n",
    "\n",
    "    plt.subplot(1,4,ifunc+1)\n",
    "    plt.plot(xs, fs, '-', c=color_cycle[0]);\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y');\n",
    "    \n",
    "    plt.plot(a, fa, 'o', ms=4, c=\"red\")\n",
    "    plt.plot(b, fb, 'o', ms=4, c=\"red\")\n",
    "\n",
    "    idx1, idx2 = [0,1] if ifunc < 3 else [1,0]\n",
    "    \n",
    "    plt.plot(chord_x, chord_y, c=cols[idx1])\n",
    "    plt.plot(chord_x, f(chord_x), c=cols[idx2])\n",
    "    \n",
    "    plt.vlines(x=a, ymin=fa, ymax=0, ls=':', colors='gray')\n",
    "    plt.vlines(x=b, ymin=fb, ymax=0, ls=':', colors='gray')\n",
    "    plt.xticks([a, b], ['a', 'b']);\n",
    "    plt.title(formula, color=cols[idx2]);\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fbba210-01ea-4e0c-9871-9aa06df869c3",
   "metadata": {},
   "source": [
    "### _Intermezzo 2: Jensen's inequality_\n",
    "\n",
    "Convex functions satisfy <em>Jensen's inequality</em> (Proof: see solution of Mackay Ex. 2.14, page 41).\n",
    "$$\n",
    "f\\left(\\sum_{i=1}^M \\lambda_i x_i \\right) \\leq \\sum_{i=1}^M \\lambda_i f(x_i)\n",
    "$$\n",
    "where $\\lambda_i \\geq 0$, $\\sum_i \\lambda_i = 1$, for any set points $x_i$.\n",
    "\n",
    "In other words:\n",
    "$$f(\\langle x \\rangle) \\leq \\langle f(x)\\rangle$$\n",
    "\n",
    "\n",
    "We can use Jensen's inequality to show that $\\mathrm{KL}(p||q)\\ge 0$ (Proof: see solution of Mackay Ex. 2.26, page 44).\n",
    "\n",
    "Use $\\lambda_i = p_i$, making use of the fact that $-\\ln(x)$ is convex:\n",
    "$$\n",
    "\\mathrm{KL}(p||q) = - \\sum_i p_i \\ln \\left( \\frac{q_i}{p_i} \\right) \\geq\n",
    "-\\ln \\left( \\sum_i p_i \\frac{q_i}{p_i} \\right) = -\\ln \\left(\\sum_i q_i\\right) = 0\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3949db87-d2d4-41e2-a50f-d8eeacfcd32e",
   "metadata": {},
   "source": [
    "## Back to Variational Approximation\n",
    "\n",
    "We can thus use KL to define a Variational Approximation:\n",
    "$$\n",
    "\\theta =\\text{argmin}_\\theta KL(q||p)\\qquad KL(q||p)=\\sum_x q(x) \\log q(x) -\\sum_x q(x) \\log p(x)\n",
    "$$\n",
    "\n",
    "The variational approximation is extensively used in ML, as we will discuss when we treat the EM algorithm and variational EM algorithms later.\n",
    "\n",
    "Note that by using $KL(q||p)$ we need to compute expectations w.r.t $q(x)$ and not $p(x)$, which is complex to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76c92d9-4a73-47ef-b1f1-048748ee5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compute KL divergence\n",
    "def compute_kl(p0, q0):\n",
    "    # renormalize distributions\n",
    "    p = p0 / p0.sum()\n",
    "    q = q0 / q0.sum()\n",
    "    plogpq = (p * np.log(p/q))\n",
    "    KL = plogpq[~np.isnan(plogpq)].sum() # deal with log(1/0)\n",
    "    return KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5e9c4-c8b8-41b4-8b31-5eefe9ca0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how to use KL to measure distance between distributions...\n",
    "# ... and how to treat normalization with care\n",
    "\n",
    "# generate probabilities from two Poisson distributions with different rates\n",
    "mus = [3, 5, 20]\n",
    "\n",
    "mean_gauss = 20\n",
    "\n",
    "# compute gaussian densities\n",
    "ks = np.arange(-10, 50)\n",
    "gaussian = multivariate_normal(mean=mean_gauss, cov=mean_gauss)\n",
    "fs_gauss = gaussian.pdf(ks) # gaussian densities\n",
    "ps_gauss = gaussian.cdf(ks + 0.5) - gaussian.cdf(ks - 0.5) # gaussian probabilities\n",
    "\n",
    "# get a gaussian with the same mean but a different variance\n",
    "gaussian_v = multivariate_normal(mean=mean_gauss, cov=7)\n",
    "fs_gauss_v = gaussian_v.pdf(ks) # gaussian densities\n",
    "ps_gauss_v = gaussian_v.cdf(ks + 0.5) - gaussian_v.cdf(ks - 0.5) # gaussian probabilities\n",
    "# compute KL\n",
    "KL_f_v = compute_kl(fs_gauss, fs_gauss_v) # using densities\n",
    "KL_p_v = compute_kl(ps_gauss, ps_gauss_v) # using probabilites\n",
    "KL_f_v_scipy = entropy(fs_gauss, fs_gauss_v) # using densities\n",
    "KL_p_v_scipy = entropy(ps_gauss, ps_gauss_v) # using probabilites\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(ks, fs_gauss, '.-', ms=3, c=\"black\");\n",
    "plt.plot(ks, ps_gauss, '.:', ms=3, c=\"black\");\n",
    "\n",
    "plt.plot(ks, fs_gauss_v, '.:', ms=3, c=\"gray\", label=f\"gauss\")\n",
    "plt.plot(ks, ps_gauss_v, '.-', ms=3, c=\"gray\");\n",
    "\n",
    "KLs_f, KLs_p = np.zeros(len(mus)), np.zeros(len(mus))\n",
    "KLs_f_scipy, KLs_p_scipy = np.zeros(len(mus)), np.zeros(len(mus))\n",
    "for imu, mu in enumerate(mus):\n",
    "    \n",
    "    # compute probs and plot distribution\n",
    "    ps_pois = poisson.pmf(ks, mu=mu)\n",
    "    plt.plot(ks, ps_pois, '.:', c=color_cycle[imu], ms=3, label=f\"poiss $\\mu$ = {mu}\");\n",
    "\n",
    "    # compute KL divergence with Gaussian (the wrong way)\n",
    "    KL_f = compute_kl(ps_pois, fs_gauss) # do it yourself\n",
    "    KL_f_scipy = entropy(ps_pois, fs_gauss) # be lazy and let scipy do it\n",
    "    KLs_f[imu] = KL_f\n",
    "    KLs_f_scipy[imu] = KL_f_scipy\n",
    "\n",
    "    # compute KL divergence with Gaussian (the correct way)\n",
    "    KL_p = compute_kl(ps_pois, ps_gauss) # do it yourself\n",
    "    KL_p_scipy = entropy(ps_pois, ps_gauss) # be lazy and let scipy do it\n",
    "    KLs_p[imu] = KL_p\n",
    "    KLs_p_scipy[imu] = KL_p_scipy\n",
    "    \n",
    "plt.xlabel('k')\n",
    "plt.ylabel('p(k)')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(122)\n",
    "# KL with gaussian\n",
    "plt.plot(20, KL_f_v, 'x', label=\"KL gauss approx\")\n",
    "plt.plot(20, KL_p_v, 'x')\n",
    "# plt.plot(20, KL_f_v_scipy, 'x') # check calculation against scipy\n",
    "# plt.plot(20, KL_p_v_scipy, 'x') # check calculation against scipy\n",
    "# KL with poisson\n",
    "plt.plot(mus, KLs_f, '.-', label=\"KL Poisson approx\")\n",
    "plt.plot(mus, KLs_p, '.-')\n",
    "# plt.plot(mus, KLs_f_scipy, '.:') # check calculation against scipy\n",
    "# plt.plot(mus, KLs_p_scipy, '.:') # check calculation against scipy\n",
    "plt.xlabel(\"$\\mu$\")\n",
    "plt.ylabel(\"KL\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be2726-1ab8-4ddd-bbda-7ada64861f8f",
   "metadata": {},
   "source": [
    "### Example: approximating an elongated Gaussian with an isotropic one\n",
    "\n",
    "Consider $p$ a elongated Gaussian distribution. Approximate with the simpler factorized distribution $q(z_1,z_2)=q(z_1)q(z_2)$. \n",
    "\n",
    "The variational approximation tends to under estimate the variance (Assigment 1.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f9350-8f07-4067-b1d5-d51f9a3c433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define true multivariate gaussian\n",
    "mean = np.array([0.,0.])\n",
    "cov = np.array([[1., 0.9],\n",
    "                [0.9, 1.]])\n",
    "gaussian = multivariate_normal(mean=mean, cov=cov)\n",
    "detcov = np.linalg.det(cov)\n",
    "invcov = np.linalg.inv(cov)\n",
    "\n",
    "# compute variances of variational approximations\n",
    "var_kl = 1/invcov[0,0]\n",
    "var_kl_reverse = cov[0,0]\n",
    "gaussian_kl = multivariate_normal(mean=mean, cov=np.array([[var_kl, 0.], [0., var_kl]]))\n",
    "gaussian_kl_reverse = multivariate_normal(mean=mean, cov=np.array([[var_kl_reverse, 0.], [0., var_kl_reverse]]))\n",
    "\n",
    "# prepare 2d grid of points\n",
    "dx = 0.05\n",
    "lim_std = 2\n",
    "lims_x = np.array([mean[0] - lim_std, mean[0] + lim_std])\n",
    "lims_y = np.array([mean[1] - lim_std, mean[1] + lim_std])\n",
    "xs = np.arange(lims_x[0], lims_x[1], dx)\n",
    "ys = np.arange(lims_y[0], lims_y[1], dx)\n",
    "xv, yv = np.meshgrid(xs, ys)\n",
    "x_stacked = np.dstack([xv, yv])\n",
    "\n",
    "# compute density\n",
    "pbins = gaussian.pdf(x_stacked)\n",
    "pbins_kl = gaussian_kl.pdf(x_stacked)\n",
    "pbins_kl_reverse = gaussian_kl_reverse.pdf(x_stacked)\n",
    "\n",
    "# plot stuff\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# plot histogram and joint\n",
    "plt.subplot(121)\n",
    "plt.imshow(pbins, origin=\"lower\", extent=[xs[0], xs[-1], ys[0], ys[-1]], alpha=0.7, cmap=\"coolwarm\", aspect=\"equal\");\n",
    "plt.contour(xv, yv, pbins, levels=3);\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contour(xv, yv, pbins, levels=3);\n",
    "plt.contour(xv, yv, pbins_kl, levels=3, cmap='Reds');\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24553955-a253-46a1-a715-35b7517e5c85",
   "metadata": {},
   "source": [
    "### The reverse $KL(p||q)$ estimates the marginal variance correctly\n",
    "\n",
    "It is quite easy to understand how the reverse KL estimates the variational approximation. \n",
    "\\begin{align}\n",
    "KL(p||q)&=\\int dx_1\\ldots dx_n p(x_1,\\ldots,x_n) \\log \\frac{p(x_1,\\ldots,x_n)}{q_1(x_1)\\ldots q_n(x_n)}\\\\\n",
    "&=\\int dx_1\\ldots dx_n p(x_1,\\ldots,x_n) \\left(\\log p(x_1,\\ldots,x_n) - \\sum_{i=1}^n \\log q_i(x_i)\\right)\\\\\n",
    "&=\\text{const}-\\sum_{i=1}^n \\int dx_1\\ldots dx_n p(x_1,\\ldots,x_n)\\log q_i(x_i)\\\\\n",
    "&=\\text{const}-\\sum_{i=1}^n \\int dx_i p_i(x_i)\\log q_i(x_i)\\\\\n",
    "&=\\text{const}+\\sum_{i=1}^n KL(p_i||q_i)\n",
    "\\end{align}\n",
    "\n",
    "So the inverse KL solution is $q_i=p_i$, with $p_i(x_i)=\\int dx_{\\setminus i} p(x_i,x_{\\setminus i})$ the marginal distribution of $p$ on $x_i$.\n",
    "\n",
    "##### Note that expectations w.r.t $p(x)$ are hard to compute in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de9b63-301f-4278-ad84-615a83986153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot stuff\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# plot histogram and joint\n",
    "plt.subplot(121)\n",
    "plt.imshow(pbins, origin=\"lower\", extent=[xs[0], xs[-1], ys[0], ys[-1]], alpha=0.7, cmap=\"coolwarm\", aspect=\"equal\");\n",
    "plt.contour(xv, yv, pbins, levels=3);\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contour(xv, yv, pbins, levels=3);\n",
    "plt.contour(xv, yv, pbins_kl_reverse, levels=3, cmap='Reds')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab72bc4a-885c-4d9e-b420-a22c4ca291b6",
   "metadata": {},
   "source": [
    "### _Another Intermezzo: Maximum Likelihood_\n",
    "\n",
    "Given a data set $D=(x_1,\\ldots,x_N$) and model $q(x|\\theta)$, a common approach to estimate $\\theta$ is the so-called **maximum likelihood** method. \n",
    "\n",
    "Let us assume each data point is generated independently from $q(x|\\theta)$. The probability to observe the entire data set $D$ is\n",
    "$$\n",
    "q(D|\\theta) = \\prod_{i=1}^N q(x_i|\\theta)\n",
    "$$\n",
    "The best value of $\\theta$ is given by maximzing $q(D|\\theta)$ w.r.t. $\\theta$. Equivalently, one maximizes the log likelihood\n",
    "$$\n",
    "L =\\frac{1}{N}\\log q(D|\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\log q(x_i|\\theta)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90c20907-0565-45a5-baf7-f5265ef297f1",
   "metadata": {},
   "source": [
    "### _KL divergence and Maximum Likelihood_\n",
    "\n",
    "We can understand the maximum likelihood method in terms of the KL divergence.\n",
    "\n",
    "Define the <em>empirical distribution</em>\n",
    "$$\n",
    "p(x) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{x,x_i} \n",
    "$$\n",
    "Then\n",
    "$$\n",
    "KL(p||q) = \\sum_x p(x)\\log \\frac{p(x)}{q(x|\\theta)} = -\\sum_x p(x)\\log q(x|\\theta) + \\text{const} =-\\frac{1}{N}\\sum_{i=1}^N \\log q(x_i|\\theta) \n",
    "$$\n",
    "Thus, minimizing $KL(p||q)$  is equivalent to maximizing the log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b90cf-64c8-40b4-a2b0-ddb492a766c6",
   "metadata": {},
   "source": [
    "# <center> Assignments </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec90dce-9955-4756-a7dc-908498765d47",
   "metadata": {},
   "source": [
    "#### Ex 1.4\n",
    "\n",
    "Show that the $n$-dimensional multivariate Gaussian distribution\n",
    "$$\n",
    "p\\left(x|\\mu,\\Sigma\\right)=\\frac{1}{\\sqrt{\\left(2\\pi\\right)^{n}\\det\\Sigma}}\\exp\\left[-\\frac{1}{2}\\left(x-\\mu\\right)^{T}\\Sigma^{-1}\\left(x-\\mu\\right)\\right]\n",
    "$$\n",
    "is an exponential family distribution. In particular, the multivariate\n",
    "Gaussian distribution is the a maximum entropy solution. What are\n",
    "the constraints?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9442bee-65d8-4774-953b-5fa9788cdf67",
   "metadata": {},
   "source": [
    "#### Ex 1.5\n",
    "\n",
    "Consider a two-dimensional Gaussian distribution with equal variances:\n",
    "$$\n",
    "p\\left(x_{1},x_{2}\\right)\\propto\\exp\\left[-\\frac{1}{2}\\left(ax_{1}^{2}+ax_{2}^{2}+2bx_{1}x_{2}\\right)\\right]\n",
    "$$\n",
    "where obviously $\\left\\langle x_{i}\\right\\rangle _{p}=0$ and $\\Sigma=\\left\\langle xx^{T}\\right\\rangle _{p}=\\left(\\begin{array}{cc}\n",
    "a & b\\\\\n",
    "b & a\n",
    "\\end{array}\\right)^{-1}$. We would like to approximate it with a product of two-Gaussians:\n",
    "$$\n",
    "q\\left(x_{1},x_{2}\\right)=q_{1}\\left(x_{1}\\right)q_{2}\\left(x_{2}\\right)\n",
    "$$\n",
    "with\n",
    "$$\n",
    "q_{i}\\left(x\\right)=\\frac{e^{-\\frac{x^{2}}{2\\sigma_{i}^{2}}}}{\\sqrt{2\\pi\\sigma_{i}^{2}}}\n",
    "$$\n",
    "We now take the following steps:\n",
    "\n",
    "* Show that $\\int dxq_{i}\\left(x\\right)\\log q_{i}\\left(x\\right)=-\\log\\left(\\sqrt{2\\pi\\sigma_{i}^{2}}\\right)-\\frac{1}{2}$\n",
    "* Use the previous results to show that\n",
    "$$\n",
    "KL\\left(q||p\\right)=-\\sum_{i=1}^{2}\\log\\left(\\sqrt{2\\pi\\sigma_{i}^{2}}\\right)+\\frac{a}{2}\\sum_{i=1}^{2}\\sigma_{i}^{2}+\\text{const}\n",
    "$$\n",
    "* Show that the variational solution for $q_{i}$ has $\\sigma_{i}^{2}=a^{-1}$\n",
    "* Show that the reverse variational solution solution is given by $\\sigma_{i}^{2}=\\left\\langle x_{i}^{2}\\right\\rangle _{p}$\n",
    "* Show that $\\left\\langle x_{i}^{2}\\right\\rangle _{p}=\\frac{a}{a^{2}-b^{2}}$\n",
    "and therefore the variance of the reverse solution is large, since\n",
    "$\\frac{1}{a}<\\frac{a}{a^{2}-b^{2}}$ (unless $b=0$, in which case\n",
    "$p$ is already factorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f3e10-a1d6-4378-8667-027a1e4d7e13",
   "metadata": {},
   "source": [
    "## Additional recommended exercises for Lecture 1\n",
    "\n",
    "Mackay Ex. 2.14, 2.16ab, 2.18, 2.19, 2.26\n",
    "\n",
    "You can find solutions in the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
