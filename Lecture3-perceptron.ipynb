{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571c1a72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "import perceptron_code\n",
    "from utils import color_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5e92a-e6d6-4bab-ad60-e8f147e97c59",
   "metadata": {},
   "source": [
    "# Lecture 3: Perceptron\n",
    "\n",
    "* Perceptron and linear separation\n",
    "* Convergence of perceptron learning rule\n",
    "* Capacity of the perceptron\n",
    "* Generalization\n",
    "\n",
    "_Recommended readings_:\n",
    "\n",
    "* Hertz, Krogh, Palmer, & Horner, Introduction to the theory of neural computation (1991)\n",
    "* Raul Rojas' [Neural Networks: A systematic introduction](https://link.springer.com/book/10.1007/978-3-642-61068-4) (1996)\n",
    "* Michael Biehl's [The shallow and the deep](https://www.cs.rug.nl/~biehl/mpubl.html) (2023). Chapter 3 contains a very detailed account of perceptron learning, with additional training methods using concepts from convex optimization.\n",
    "* Mohri, Rostamizadeh, Talwalkar, [Foundations of Machine Learning](https://cs.nyu.edu/~mohri/mlbook/). Chapter 2 and 3 contain a very readable account of the PAC framework and VC dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973362f-df07-4e2c-bad3-f8371e7962ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The Perceptron\n",
    "\n",
    "$\\xi=(\\xi_1,...,\\xi_d)$ is an array of $d$ input pixels, i.e., $\\xi$ is the image.\n",
    "$\\phi_i(\\xi), i=1,...,N-1$ is an array of (given) features computed for each image $\\xi$.\n",
    "$w=(w_0,w_1,... w_{N-1})$ is an array of $n$ adaptable parameters.\n",
    "\n",
    "Given $\\xi,w$ compute\n",
    "$$y = \\text{sign}\\left(\\sum_{i=1}^{N-1} w_i \\phi_i(\\xi) + w_0 \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83835e72-6fd1-4969-9702-c1d484969bff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The Perceptron\n",
    "\n",
    "##### Learning problem:\n",
    "Given a training set of images $\\xi^\\mu, \\mu = 1,... P$, where each image belongs to one of two classes labeled by $t^\\mu =\\pm 1$, find $w$ such that\n",
    "$$\\text{sign}\\left(\\sum_{i=0}^{N-1} w_i \\phi_i^\\mu\\right)=t^\\mu,\\qquad \\mu=1,...,P$$\n",
    "where we write $\\phi_i^\\mu = \\phi_i(\\xi^\\mu)$ and we have defined $\\phi_0^\\mu=1$.\n",
    "\n",
    "Equivalently, we demand\n",
    "$$\\text{sign}\\left(\\sum_{i=0}^{N-1} w_i^\\mu \\phi_i^\\mu t^\\mu\\right)=1\\qquad\\text{or}\\quad w^T\\cdot x^\\mu >0$$\n",
    "with $x_i^\\mu=\\phi_i^\\mu t^\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd445652-9712-413d-855e-6b15d4df50de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Linear separation\n",
    "\n",
    "Classification depends on the sign of $w\\cdot x$. Thus, the decision boundary is an $N-1$ dimensional hyperplane defined by:\n",
    "$$0=\\sum_{j=1}^{N-1} w_j \\phi_j +w_0$$\n",
    "All points of one class should be on one side of the hyperplane, and all others on the other side.\n",
    "Thus, the perceptron can solve so-called **linearly separable problems**.\n",
    "\n",
    "Examples: in 2 dimensions, the AND problem is linearly separable, whereas the XOR problem is not linearly separable.\n",
    "\n",
    "In the following, we will consider points in **general position** in $N$ dimension, so that the bias $w_0$ is not a special weight and we index weight from $1$ to $N$ to reduce clutter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef981c28-bac0-4f12-8d89-9b57316d8bb7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Perceptron learning rule\n",
    "\n",
    "Learning is successful when\n",
    "$$w\\cdot x^\\mu > 0,\\quad \\text{all patterns $\\mu$}$$\n",
    "Consider the following learning mechanism, where the weights are updated when a single input pattern $\\phi^\\mu$ and its output label $t^\\mu$ are presented to the perceptron:\n",
    "\n",
    "1.  Compute $x_i^\\mu = \\phi_i^\\mu t^\\mu$.\n",
    "2.  When $w$ is such that $w \\cdot x^\\mu >0$ do nothing.\n",
    "3.  else $w \\to w+\\eta x^\\mu $.\n",
    "\n",
    "$\\eta>0$ is called the _learning rate_.\n",
    "\n",
    "Note that $w \\cdot x^\\mu$ increases after update 3, indeed $w\\cdot x^\\mu \\to (w+\\eta x^\\mu)\\cdot x^\\mu=w \\cdot x^\\mu + \\eta |x^\\mu|^2> w \\cdot x^\\mu $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03db95-d983-4821-90c7-848cc273113c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Perceptron learning rule\n",
    "\n",
    "We can thus write the perceptron learning rule as:\n",
    "\n",
    "$$w_i^\\mathsf{new}=w_i^\\mathsf{old}+\\Delta w_i$$\n",
    "$$\\Delta w_i=\\eta\\Theta(-w\\cdot x^\\mu)x^\\mu_i$$\n",
    "\n",
    "where we used the [Heaviside function](https://en.wikipedia.org/wiki/Heaviside_step_function): $\\Theta(x)=1$ if $x>0$ and $\\Theta(x)=0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10287b27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Perceptron learning rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d4343-4c7e-4807-a69c-d9e42cb14605",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "alpha = 1.2\n",
    "P = int(alpha * N)\n",
    "learn_bias = False\n",
    "\n",
    "X = np.random.randn(P, N)\n",
    "y = np.random.randint(2, size=P)\n",
    "\n",
    "if learn_bias:\n",
    "    X = np.c_[X, np.ones((P,1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d427fbe",
   "metadata": {},
   "source": [
    "### Sequential implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6928958f-2ab4-420e-8923-0c8bc77bf770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... you will write your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a896f5-2727-4c0c-9f73-e553e478660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "lr = 1.\n",
    "num_epochs = 1000\n",
    "print_every = 100\n",
    "\n",
    "\n",
    "w, err, errs, ep = perceptron_code.train_perceptron(X, y, learn_bias=learn_bias, lr=lr, num_epochs=num_epochs,\n",
    "                                                    print_every=print_every, verbose=True, parallel=False)\n",
    "\n",
    "plt.plot(errs, '.-');\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel(\"errors\");\n",
    "# plt.xscale('log');\n",
    "plt.yscale('log');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0571c615-e3bc-464b-b2f5-6e27d42341b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A parallel version of the perceptron learning rule\n",
    "\n",
    "The perceptron learning rule is sequential, updating one pattern at a time, and requires a for loop.\n",
    "\n",
    "```\n",
    "for mu = 1: P\n",
    "...\n",
    "end;\n",
    "```\n",
    "\n",
    "You can make a faster parallel version:\n",
    "\n",
    "1.  Find the set $S$ of training patterns with wrong output.\n",
    "2.  Update $w$ using all patterns found in step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1a78b",
   "metadata": {},
   "source": [
    "### parallel implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d66577-135e-487d-a0bc-c71ead8b9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... you will write your own code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff32e16-16f7-47a3-af5b-b0620d55d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, err, errs, ep = perceptron_code.train_perceptron(X, y, learn_bias=learn_bias, lr=lr, num_epochs=num_epochs,\n",
    "                                                    print_every=print_every, verbose=True, parallel=True)\n",
    "\n",
    "plt.plot(errs, '.-');\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel(\"errors\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dd5522f-4fb9-4cc9-b985-f89a98732f04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Convergence of the perceptron learning rule\n",
    "\n",
    "Depending on the data, there may be many or few solutions to the learning problem (or none at all).\n",
    "\n",
    "Any $w$ for which $ w \\cdot x^\\mu>0$ for all $\\mu$ is a solution to the learning problem, or\n",
    "$$\\min_\\mu w\\cdot x^\\mu >0$$Note, that the perceptron does not depend on the magnitude of $w$. Thus, we define\n",
    "$$D(w)=\\frac{1}{\\|w\\|}\\min_\\mu w \\cdot x^\\mu$$\n",
    "\n",
    "where $\\|w\\|^2=\\sum_{i=1}^{n} w_i^2$.\n",
    "\n",
    "Better solutions have a larger $D(w)$: $\\frac{w}{\\|w\\|} \\cdot x^\\mu$ is the distance of the point $x^\\mu$ from the hyperplane orthogonal to $w$.\n",
    "\n",
    "We thus define the best solution as $D_\\text{max}=\\max_w D(w)$. When $D_\\text{max}>0$ the problem is linearly separable, otherwise it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae71bf-cb43-4c7a-87f4-df58ceeae4ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Convergence of Perceptron rule\n",
    "\n",
    "Assume that the problem is linearly separable, so that there is a solution $w^*$ with $D(w^*)>0$.\n",
    "\n",
    "Suppose we performed $t$ iterations of the learning rule. In each iteration, a pattern $\\mu$ was presented that either changed $w$ or not.\n",
    "\n",
    "Denote by $M^\\mu$ the number of times pattern $\\mu$ has been presented and caused a non-zero update of $w$. If we start with the initial value $w=0$, after $t$ iterations\n",
    "$$w=\\eta \\sum_\\mu M^\\mu x^\\mu$$\n",
    "\n",
    "Denote by $M=\\sum_\\mu M^\\mu$ the total number of non-zero updates of $w$. We will prove that\n",
    "$$A(w)=\\frac{w \\cdot w^*}{\\|w\\|\\|w^*\\|} \\ge \\mathcal{O}(\\sqrt{M})$$\n",
    "\n",
    "If the learning rule does not converge, $w$ will be updated forever, $M\\to \\infty$, and $A\\to \\infty$.\n",
    "\n",
    "However, $A(w)=\\cos\\theta\\le 1$ and cannot grow forever. The conclusion is that $M$ must stay finite. This means that after a finite number of updates of $w$ there are no more changes.\n",
    "\n",
    "Thus, the perceptron learning rule converges in a finite number of steps when the problem is linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af83f3-621a-4fc6-be9e-71fbd796e795",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Convergence of Perceptron rule\n",
    "\n",
    "We can bound\n",
    "$$w\\cdot w^*=\\eta \\sum_\\mu M^\\mu x^\\mu \\cdot w^* \\ge \\eta\\sum_\\mu M^\\mu \\min_\\nu x^\\nu \\cdot w^* =\\eta M \\min_\\nu x^\\nu \\cdot w^* = \\eta M D(w^*) \\|w^*\\|$$\n",
    "\n",
    "Consider the change of $|w|^2_\\mu$ when updating with a single pattern $\\mu$:\n",
    "\n",
    "$$\\Delta \\|w\\|^2_\\mu=\\|w+\\eta x^\\mu\\|^2-\\|w\\|^2=2\\eta w \\cdot x^\\mu + \\eta^2 \\|x^\\mu\\|^2\\le \\eta^2 \\|x^\\mu\\|^2$$\n",
    "\n",
    "where in the inequality we used that $w\\cdot x^\\mu<0$, because otherwise there would have been no update and $\\Delta \\|w\\|_\\mu^2=0$. Thus, at iteration $t$:\n",
    "$$\\|w\\|^2=\\sum_\\mu M^\\mu \\Delta \\|w\\|^2_\\mu\\le \\eta^2 \\sum_\\mu M^\\mu \\|x^\\mu\\|^2 \\le \\eta^2 M B$$\n",
    "or equivalently\n",
    "$$\\|w\\|\\le \\eta \\sqrt{MB}$$\n",
    "where we define $B=\\max_\\mu \\|x^\\mu\\|^2$, which is a constant.\n",
    "Thus,\n",
    "$$A(w)=\\frac{w\\cdot w^*}{\\|w\\|\\|w^*\\|} \\ge \\sqrt{\\frac{M}{B}}D(w^*)$$\n",
    "\n",
    "We can invert this relation to bound the total number of weight updates:\n",
    "$$M \\le \\frac{B}{D^2(w^*)}\\propto \\frac{N}{D^2(w^*)}$$\n",
    "since $B=\\max_\\mu |x^\\mu|^2=\\mathcal{O}(N)$. The convergence is slower for higher dimensional problems and also for harder problems, for which $D(w^*)\\gtrsim 0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91abe454-e063-4e9b-9a9f-e76953515d8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Capacity of the Perceptron\n",
    "\n",
    "The perceptron can only learn linearly separable problems. How often will one encounter a linearly separable problem in practice? In the following analysis we will consider random problems for which we can compute some answers.\n",
    "\n",
    "Consider $P$ patterns in $N$ dimensions (we are interested in the large $N$ behaviour and therefore we ignore the component $\\phi_0=1$ and $w_0$):\n",
    "$$\\phi^\\mu=(\\phi_1^\\mu,...,\\phi_N^\\mu),\\quad \\mu=1,...,P$$\n",
    "\n",
    "We assume that the $N$-dimensional vectors $\\phi^\\mu$ are in **general position**: any subset of $N$ or fewer points are linearly independent. Such condition which avoids situations where a subset of patterns are exactly on a linear subspace. When $\\phi^\\mu$ are generated at random, they are in general position **with probability one**.\n",
    "\n",
    "We assign each pattern $\\phi^\\mu$ a random label $t^\\mu=+1$ or $t^\\mu=-1$. This defines a binary classification problem. What is the probability that this problem is linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58150670-a184-477e-9556-14b46069ed2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Capacity of the Perceptron\n",
    "\n",
    "Each pattern can be either $+1$ or $-1$, therefore each set of $P$ patterns defines $2^P$ classification problems.\n",
    "\n",
    "The perceptron defines a hyperplane in $N$ dimensions through the origin $\\sum_{i=1}^N w_i \\phi_i=0$. Which fraction of the $2^P$ possible classification problems is linearly separable?\n",
    "\n",
    "Examples\n",
    "\n",
    "  * $N=2,P=2$. The total number of problems is $2^P=4$. They are all linearly separable.\n",
    "  * $N=2, P=3$. The total number of problems is $2^P=8$. There are 2 problems that are not linearly separable.\n",
    "  * $N=3, P=3$. The total number of problems is $2^P=8$. They are all linearly separable.\n",
    "  * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c0a07-85eb-48f4-b9cf-9d1e6a20f706",
   "metadata": {},
   "source": [
    "Example of a separable (left) and non separable (right) problem in $N=2$ dimension with $P=3$ points:\n",
    "\n",
    "<center><img src=\"figs/dichotomy_2d.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb3c26-7954-427e-bef8-14d6d9f96461",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Capacity of the Perceptron\n",
    "\n",
    "#### Theorem (Cover 1966)\n",
    "The number of linearly separable labelings of $P$ points in $N$ dimensions with separability plane through the origin is:\n",
    "$$C(P,N)=2 \\sum_{i=0}^{N-1}\\binom{P-1}{i}$$\n",
    "\n",
    "In the exercise you will show that:\n",
    "* when $P\\le N$, then $C(P,N)=2^P$: all problems are linearly separable;\n",
    "* when $P=2N$, then $C(P,N)=2^{P-1}$: 50% of all problems are linearly separable.\n",
    "\n",
    "When $P,N\\to \\infty$ there is a sharp transition at $P=2N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f2545-bcb1-463e-9144-221fefcb316a",
   "metadata": {},
   "source": [
    "The probability for a random labeling of $P$ points in $N$ dimensions to be linearly separable is thus expressed by $C(P,N)/2^P$.\n",
    "\n",
    "The transition to a negligible probability is sharper and sharper as $N$ and $P$ grow. In the figure $N = 5$ (large dots), $N = 50$ (small dots) and $N = 500$ (line)\n",
    "\n",
    "<center><img src=\"figs/prob_linear_separability.png\" width=600></center>\n",
    "\n",
    "[figure taken from _Engel, Van den Broeck, Statistical mechanics of learning (2001)_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440133b5-4ff1-4e9c-b832-bdc9e0feae0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Proof of Theorem\n",
    "#### By induction\n",
    "\n",
    "Suppose the number of linearly separable problem with $P$ patterns in $N$ dimensions is $C(P,N)$. In the image we have $P=4$, the two blue and two red points.\n",
    "\n",
    "<center><img src=\"figs/perceptron_theorem_points.png\" width=400></center>\n",
    "\n",
    "We add one point $X$ (the green point) and wish to compute $C(P+1,N)$.\n",
    "The set $C(P,N)$ consists of linearly separable problems, which we can divide in two subsets:\n",
    "\n",
    "  * **Set A** (left panel): a separating hyperplane that can be drawn through $X$. For each of these problems we can define two new linearly separable problems by coloring the green point either red or blue, by slightly rotating the separating plane one way or the other [Note that no other point will cross the hyperplane due to such small rotation, since the points are in general position];\n",
    "  * **Set B** (right panel): For each of these problems we can define only one separable problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414378b7-9c67-49ed-9328-8f365932f788",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We thus have:\n",
    "$$C(P+1,N)=2 A + B= (A+B)+A=C(P,N) + A$$\n",
    "\n",
    "The set $A$ is the set of linearly separable problems on $P$ points in $N$ dimensions, where the separating hyperplane goes through the origin $O$ and through the point $X$. We can count the size of $A$ by projecting points on the hyperplane orthogonal to the line $O-X$, thereby reducing the dimensions to $N-1$. Therefore we have the recurrence relation:\n",
    "$$A=C(P,N-1)$$Thus$$C(P+1,N)=C(P,N)+C(P,N-1)$$\n",
    "\n",
    "One can then start from the two obvious relations\n",
    "$$C(1,N)=2$$ (any hyperplane not passing through a single point can assign it a label $+1$ or $-1$, depending on its orientation) and $$C(P,1)=2$$ (the only available hyperplane passing through the origin in $1$ dimension is the origin) to arrive at\n",
    "$$C(P,N)=2 \\sum_{i=0}^{N-1}\\binom{P-1}{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304406b-2ac8-484f-be66-1e0ff7e12f61",
   "metadata": {},
   "source": [
    "We can check that the expression respects the recursion. Indeed:\n",
    "\\begin{align*}\n",
    "C(P,N)+C(P,N-1)= & 2\\sum_{i=0}^{N-1}\\binom{P-1}{i}+2\\sum_{i=0}^{N-2}\\binom{P-1}{i}=\\\\\n",
    " & 2\\binom{P-1}{0}+2\\sum_{i=1}^{N-1}\\left[\\binom{P-1}{i}+\\binom{P-1}{i-1}\\right]=\\\\\n",
    " & 2\\sum_{i=0}^{N-1}\\binom{P}{i}=C(P+1,N)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc9cb5-3576-47c9-a705-3580c70e8ec5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generalization\n",
    "\n",
    "Given inputs $x=(x_1,...,x_n)$ and binary output $y$.\n",
    "Suppose we wish to learn a given, but unknown, function $\\bar{f}: x\\to y$ from data.\n",
    "Denote our solution as $f:x\\to y$.\n",
    "\n",
    "An example is that $\\bar{f}$ is some linearly separable classifier from which we have some data.\n",
    "$f$ is our perceptron solution.\n",
    "\n",
    "Define the generalization performance of any function $f$ as\n",
    "$$g(f)=\\text{Prob}(f(x)=\\bar{f}(x))\\qquad \\text{$x$ is uniform random}$$Also, define the performance of $f$ on a training set of $P$ patterns$$g_P(f)=\\text{Prob}(f(x)=\\bar{f}(x)) \\qquad \\text{$x$ is uniform from training set of size $P$}$$\n",
    "We do not know $g(f)$ but we do know $g_P(f)$ because we have the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126376f-9e6c-4a5e-b440-2e3ef2b37046",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generalization\n",
    "\n",
    "There exists a theorem that allows us to estimate $g(f)$ from $g_P(f)$:\n",
    "$$\\text{Prob}\\left(\\max_f|g_P(f)-g(f)|> \\epsilon\\right) \\le 4 m(2P)e^{-\\epsilon^2 P/8}$$\n",
    "So, if we can make the right hand side small, say $0.01$ and we find perfect performance on the training set $g_P(f)=1$, we know that the generalization performance $g(f)> 1-\\epsilon$ with probability $0.99$.\n",
    "\n",
    "$m(P)$ is called the **growth function**: it counts the number of different binary functions on $P$ inputs that can be realized by our learning architecture.\n",
    "In the case of the perceptron $m(P)=C(N,P)$ is the number of linearly separable functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb399a4-bbf6-4190-82b9-2d308b0473b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generalization\n",
    "\n",
    "In general $m(P) = 2^P$ for $P\\le d_\\text{VC}$ and becomes polynomial in $P$ for $P> d_\\text{VC}$. $d_\\text{VC}$ is called the VC (Vapnik-Cervonenkis) dimension.\n",
    "\n",
    "Note, that we need $P \\> d_\\text{VC}$ in order to make the rhs $m(2P)e^{-\\epsilon^2 P/8}$ of the bound small.\n",
    "\n",
    "One can prove that the polynomial growth of $m(P)$ is bounded as:\n",
    "$$m(P)\\le \\left(\\frac{e P}{d_\\text{VC}}\\right)^{d_\\text{VC}}\\qquad P> d_\\text{VC}$$\n",
    "\n",
    "as a direct result of the so-called **Sauer's lemma**.\n",
    "\n",
    "For more:\n",
    "* https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension\n",
    "* Mohri, Rostamizadeh, Talwalkar, [Foundations of Machine Learning](https://cs.nyu.edu/~mohri/mlbook/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b42618-c745-42d8-8df1-a58eef632ef3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generalization for the perceptron\n",
    "\n",
    "For the perceptron $m(P)=C(N,P)$ and $d_\\text{VC}=N$.\n",
    "\n",
    "We can then estimate that when $P,N$ are large, we need\n",
    "$$P\\gtrsim \\frac{N}{\\epsilon^2}\\quad \\to \\quad |g_P(f)-g(f)|<\\epsilon$$\n",
    "Thus we need $\\mathcal{O}(N)$ training samples for good generalization of the perceptron (but with a pretty large prefactor $1/\\epsilon^2$\\!)\n",
    "\n",
    "#### Proof\n",
    "\n",
    "Using $d_\\text{VC}=N$, the bound on the growth factor is $m(2P)\\le \\left(\\frac{2e P}{N}\\right)^{N}$.\n",
    "The right hand side of the generalization bound\n",
    "$$4 m(2P) \\exp\\left(-\\frac{\\epsilon^2 P}{8}\\right) \\le 4\\exp \\left(N \\left(\\log(2e\\alpha) -\\frac{\\alpha \\epsilon^2}{8}\\right)\\right)$$\n",
    "with $\\alpha=P/N$.\n",
    "The right hand side is small when the term in the exponent becomes negative, which occurs for sufficiently large $\\alpha$.\n",
    "An analysis of the function $f(\\alpha)=\\log(2e\\alpha) -\\frac{\\alpha \\epsilon^2}{8}$ shows that it crosses zero when $\\alpha =\\alpha_c \\propto \\frac{1}{\\epsilon^2}$. Thus\n",
    "$$\\alpha \\gtrsim \\alpha_c \\qquad P_c = N \\alpha_c \\gtrsim \\frac{N}{\\epsilon^2}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be6d8e5c-2ed7-42e1-87d5-a4e6f8483c4c",
   "metadata": {},
   "source": [
    "# Appetizer for Advanced Machine Learning\n",
    "\n",
    "### Generalization in the teacher-student perceptron\n",
    "\n",
    "The **typical** generalization error of a **student** perceptron trained on labels generated by a **teacher** perceptron on random inputs can be computed exactly in the limit of large $N$, due to [concentration](https://en.wikipedia.org/wiki/Concentration_of_measure) in high dimension. The pioneering work of [Elizabeth Gardner](https://en.wikipedia.org/wiki/Elizabeth_Gardner_(physicist)) laid the foundations for the Statistical Mechanics of Learning, using methods from disordered systems (spin glasses) to study the typical behaviour of learning algorithms as well as fundamental limits in inference problems.\n",
    "\n",
    "Find an appetizer of part of the course Advanced Machine Learning in the following section about generalization in the **teacher-student** setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566216bb-3440-4fcd-bc77-f867a0b6c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the theory spanning over α\n",
    "\n",
    "αs = np.arange(1, 20, 1)\n",
    "Rs = np.zeros(len(αs))\n",
    "gen_errors = np.zeros(len(αs))\n",
    "\n",
    "R0 = 0.3\n",
    "for iα, α in enumerate(αs):\n",
    "    R = optimize.fsolve(perceptron_code.eqR, R0, args=(α)).item()\n",
    "    gen_errors[iα] = perceptron_code.compute_gen_error(R)\n",
    "    Rs[iα] = R\n",
    "    R0 = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9d5ff-1c17-4985-85d2-9af065cf9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate teacher vector and test patterns\n",
    "N = 200\n",
    "Ptest = 10000\n",
    "T, Xtest, ytest_sign = perceptron_code.generate_teacher_and_test_set(Ptest, N)\n",
    "\n",
    "# set parameters\n",
    "lr = 0.1\n",
    "num_epochs = 1000\n",
    "print_every = 1e10\n",
    "parallel = True\n",
    "renormalize = True\n",
    "learn_bias = False\n",
    "\n",
    "α_span = np.arange(1, 20, 1)\n",
    "w_span = np.zeros((len(α_span), N))\n",
    "normw_span = np.zeros(len(α_span))\n",
    "err_span = np.zeros(len(α_span))\n",
    "ov_span = np.zeros(len(α_span))\n",
    "gen_err_span = np.zeros(len(α_span))\n",
    "\n",
    "for iα, α in enumerate(α_span):\n",
    "\n",
    "    print(f\"doing α: {α}\")\n",
    "    \n",
    "    P = int(α * N)\n",
    "    X = np.random.randn(P, N)\n",
    "    y = 1. * (X @ T > 0)\n",
    "\n",
    "    w, err, errs, ep = perceptron_code.train_perceptron(X, y, learn_bias=learn_bias, lr=lr, num_epochs=num_epochs,\n",
    "                                                        print_every=print_every, parallel=parallel, renormalize=renormalize)\n",
    "    w_span[iα] = w\n",
    "    normw_span[iα] = (w**2).sum()\n",
    "    err_span[iα] = err\n",
    "    ov_span[iα] = w @ T / N\n",
    "    gen_err_span[iα] = 1 - (np.sign(Xtest @ w) * ytest_sign > 0).sum() / Ptest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d672f3a2-dd0a-4d64-b42c-747e2a31602e",
   "metadata": {},
   "source": [
    "#### Visualize training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a9924-34b3-44f7-80b2-8b76f568cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(α_span, err_span, '.-', color='black', label='train error');\n",
    "plt.xlabel('α');\n",
    "plt.ylabel('train error');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1687a26-cdba-4396-ad66-320d9b965e1e",
   "metadata": {},
   "source": [
    "#### Visualize overlap and generalization error again theoretical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73be605-7e69-441b-b479-d1b2a360b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(α_span, ov_span, '.', color='blue', label=\"$W \\cdot T$ / N\")\n",
    "plt.plot(αs, Rs, '--', color='blue', label=\"$W \\cdot T$ / N theory\");\n",
    "\n",
    "plt.plot(α_span, gen_err_span, '.', color='red', label=\"gen error\");\n",
    "plt.plot(αs, gen_errors, color='red', label=\"gen error theory\");\n",
    "\n",
    "plt.legend();\n",
    "plt.xlabel('α');\n",
    "plt.ylabel('overlap / gen error');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58cfa7-ca2c-40c7-81c9-50bfbf74ac26",
   "metadata": {},
   "source": [
    "# <center>Assignments</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52780516-a568-4bab-8200-35e710056064",
   "metadata": {},
   "source": [
    "#### Ex 4.1\n",
    "\n",
    "In this exercise we look at some special cases of the perceptron capacity\n",
    "$$\n",
    "C(P,N)=2 \\sum_{i=0}^{N-1}\\binom{P-1}{i}\n",
    "$$\n",
    "to better understand the behaviour of $C$.\n",
    "* Show that all problems with $P \\leq N$ are linearly separable.\n",
    "* Show that exactly half of the problems with $P=2N$ are linearly separable.\n",
    "\n",
    "Hints:\n",
    "* In the formula, the convention is $\\binom{n}{0}=1$; $\\binom{n}{k}=0$ when $n<k$ assumed.\n",
    "* The formula $(1+\\alpha)^n= \\sum_{i=0}^n \\binom{n}{i} \\alpha^i$ may come in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d425e4-a87c-4460-ab69-f374926955c4",
   "metadata": {},
   "source": [
    "#### Ex 4.2\n",
    "\n",
    "In this exercise we numerically check the formula $C(N,P)$ for the number of linearly separable problems.\n",
    "* Write a computer program that implements the perceptron learning rule. Take as data $P$ random input vectors of dimension $N$ with binary components. Take as outputs random assignments $\\pm 1$.\n",
    "* Take $N=50$. Test empirically for individual problems that when $P < 2N$ the rule converges almost always and for $P > 2N$ the rule converges almost never.\n",
    "* Reconstruct the curve $C(P,N)$ for $N=50$ as a function of $P$ in the following way. For each $P$ construct a number (`nruns`) of learning problems randomly and compute\n",
    "  1. the fraction of these problems for which the perceptron learning rule converges;\n",
    "  2. the mean and std of the classification error on the training set;\n",
    "  3. the mean and std of the number of iterations until convergence.\n",
    "\n",
    "Suggestions: Use $P=10,20,30, \\dots, 120$; Take `nruns=100`. Decide that the algorithm does not converge when 1000 iterations has been reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41299a-024b-4b16-a470-2d4d17ada048",
   "metadata": {},
   "source": [
    "#### Ex 4.3\n",
    "\n",
    "The number of linearly separable problems of $P$ patterns in $N$ dimensions is given by $C(N,P)$. We know that $C(N,P)=2^P$ when $P \\leq N$. When $P>N$ we can use the bound\n",
    "$$\n",
    "C(N,P)\\le \\left(\\frac{e P}{N}\\right)^{N}\n",
    "$$\n",
    "Compute numerically $C(N,P)$ and its bound for $N=50$ and for $P=1$ to $P=200$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a68535-3392-41a1-9628-a543e1e53f0d",
   "metadata": {},
   "source": [
    "#### Ex 4.4\n",
    "\n",
    "The generalization bound is quite conservative. In this exercise we will verify this numerically for the perceptron. We define\n",
    "$$\n",
    "\\delta = 4 m(2P) \\exp\\left(-\\frac{\\epsilon^2 P}{8}\\right)\n",
    "$$\n",
    "and put it for instance to $\\delta =0.01$. We can then ask what the error $\\epsilon$ is for given $N$ and $P$.\n",
    "We can compare this error with the generalization error that we find by numerical simulation.\n",
    "\n",
    "In particular, suppose that data is generated from a so-called **teacher perceptron**, which is specified by an $N$ dimensional weight vector $w^\\text{teacher}$. The input data are $P$\n",
    "binary vectors, each of dimension $N$. So we can define the input data as a matrix $\\xi$ of size $N \\times P$.\n",
    "We generate a training set by defining output labels $y_j = \\text{sign}\\left(\\sum_{i=1}^N \\xi_{ij} w^\\text{teacher}_i\\right)$.\n",
    "\n",
    "The training data are used to train another perceptron (the so-called **student perceptron**). By construction the problem is linearly separable and therefore the perceptron learning rule will always converge and the solution will perfectly separate the two classes. Thus, in terms of the generalization bound, the student solution implements a function $f$ with $g_P(f)=1$. The probability that the generalization performance $g(f)$ of this solution is larger than $1-\\epsilon$ is given by the generalization bound.\n",
    "\n",
    "We can get a numerical estimate of the generalization error, by generating a separate test set of $P_\\text{test}$ patterns with labels again computed from the teacher perceptron. The generalization error is the fraction of test patterns that are incorrectly classified by the student perceptron solution.\n",
    "\n",
    "The student solution $f$ is not unique. Starting with a different initial weight vector, a different converged solution $f$ is obtained. In order to get a reliable numerical estimate of the generalization error, we should run the perceptron learning rule many times with different initial weight vectors and compute the average generalization error.\n",
    "\n",
    "* Using the formula for $\\delta$ above, compute an expression of $\\epsilon$ in terms of $N$ and $P$ and $\\delta=0.01$. Approximate $m(P)=C(N,P)$ by its bound as given in **Ex 4.3**. Compute numerically for $N=10$ the dependence of $\\epsilon$ on $P$. Compute the number of patterns $P$ to ensure that $\\epsilon \\approx 0.1$.\n",
    "  Repeat this for $N=20,30,40,50$. Note that the required number of patterns scales linearly with $N$.\n",
    "\n",
    "* Estimate the generalization error for the teacher student perceptron learning scenario as described above. In more detail:\n",
    "    - Generate input training data $\\xi$ of size $N \\times P$ with $\\xi_{ij}$ binary $\\pm 1$;\n",
    "    - Define a random (but fixed) teacher vector $w^\\text{teacher}$: `w_0=randn(1,n)`;\n",
    "    - Compute the teacher labels $y_j$ as defined above;\n",
    "    - Generate in the same way a test set $\\xi_\\text{test}$ of size $N \\times P_\\text{test}$ with $P_\\text{test}=10.000$ and teacher labels;\n",
    "    - Compute `n_learning_runs=100` perceptron solutions by training on the training set with $P$ samples with different initial weight vectors $w$. After convergence, the training error should be zero ($g_P(f)=1$) but the solutions are different. Compute for each solution the generalization error on the test set $\\epsilon$. Use $N=10$ and $P=10,50,100,500,1000$;\n",
    "    - Make a table where you compare your numerical estimates for $\\epsilon$ with those given by the generalization bound ($\\delta=0.01$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
