{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d1e495-6fc8-48e1-a26c-135aff565c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.polynomial import Polynomial\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5247cb-03d9-4f0e-bc57-cfd5628c054c",
   "metadata": {},
   "source": [
    "# Lecture 2: Model comparison\n",
    "\n",
    "* **2.2 Evidence framework and model comparison**\n",
    "    * The evidence framework\n",
    "    * Laplace approximation\n",
    "    * Model comparison in curve fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c03c0-9fa1-42b8-9d3d-8b2242e27b7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Model comparison: the evidence framework\n",
    "\n",
    "Consider models $H_i$ with parameters $w_i$, priors $p(w_i|H_i)$ and likelihoods $p(D|w_i,H_i)$.\n",
    "\n",
    "For each $H_i$, we can compute the posterior distribution over $w_i$\n",
    "$$p(w_i|D,H_i)=\\frac{p(D|w_i,H_i)p(w_i|H_i)}{p(D|H_i)} \\qquad p(D|H_i)=\\int dw_i p(D|w_i,H_i)p(w_i|H_i)$$\n",
    "\n",
    "The normalization constants $p(D|H_i)$ are called the **model evidence** and appear as the likelihood in model comparison\n",
    "$$p(H_i|D)=\\frac{p(D|H_i)p(H_i)}{p(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44792cd-8550-441d-ba39-6f6472893faa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Comparing different models: Occam's razor\n",
    "\n",
    "Bayesian model comparison naturally implements **Occam's razor**:\n",
    "\n",
    "_Entia non sunt multiplicanda praeter necessitatem_ or _Entities must not be multiplied beyond necessity_\n",
    "\n",
    "Simple models explain (i.e. assign higher probability to) only few data sets (but really well\\!). Complex models explain many data sets by using the freedom of $w$.\n",
    "$$p(D| H)=\\int dw p(D|w,H) p(w|H)$$\n",
    "\n",
    "<center><img src=\"figs/model_comparison_evidence.png\" width=300></center>\n",
    "\n",
    "Since $p(D|H_2)$ is broader, $p(D|H_1)$ is higher because of normalization: $\\int dD p(D|H)=1$. Both $H_{1,2}$ and $H_{2}$ explain $D\\in C_1$, but the simpler model $H_1$ is more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe29cd2-0e4c-4f33-a41d-6576ae98793e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Comparing different models: Occam's razor\n",
    "\n",
    "<center><img src=\"figs/evidence_2d.png\" width=500></center>\n",
    "\n",
    "Three models with equal priors $p(\\mathcal{H}_i)$ but different priors $p(w|\\mathcal{H}_i)$.\n",
    "\n",
    "$w,D$ are one dimensional and $p(D|w,\\mathcal{H}_i)=\\mathcal{N}(D|w,\\epsilon)$ for all models.\n",
    "\n",
    "Joint distribution $p(D,w|\\mathcal{H}_i)=p(D|w,\\mathcal{H}_i)p(w|\\mathcal{H}_i)$ shown as as point clouds.\n",
    "\n",
    "Given particular $D$, $p(D|\\mathcal{H}_i)$ is highest for $i=2$. It is the simplest model that explains the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4695da43-2961-4c5e-a6c0-7f66e0440e3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Occam's razor: A 1-dimensional heuristic argument\n",
    "\n",
    "The posterior over parameters is always more narrow than the prior:\n",
    "$$p(w|D,\\mathcal{H}_i)\\propto p(D|w,\\mathcal{H}_i) p(w|\\mathcal{H}_i)$$\n",
    "\n",
    "<center><img src=\"figs/model_comparison_heuristic.png\" width=350></center>\n",
    "\n",
    "Assume the posterior is peaked around a value $w_\\mathrm{MP}$ with a width by $\\sigma_{w|D}$. Take for simplicity a flat prior in region $\\sigma_w$, i.e. $p(w|\\mathcal{H}_i)=\\frac{1}{\\sigma_w}$.\n",
    "\n",
    "Approximate the un-normalized posterior by its value $w_\\mathrm{MP}$, then:\n",
    "\n",
    "$$p(D|\\mathcal{H}_i)=\\int dw p(D|w,\\mathcal{H}_i)p(w|\\mathcal{H}_i)\\approx \\underbrace{p(D|w_\\mathrm{MP},\\mathcal{H}_i)}_{\\text{best fit}} \\underbrace{\\frac{\\sigma_{w|D}}{\\sigma_w}}_{\\text{Occams factor}}$$\n",
    "\n",
    "\n",
    "Occam factor is equal to the ratio of the posterior accessible volume of $\\mathcal{H}_i$’s parameter space to the prior accessible volume. \n",
    "\n",
    "Equivalently, the logarithm of the Occam factor is a measure of the amount of information we gain about the model’s parameters when the data arrive (recall that the entropy of a Gaussian is the log of its std)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1985d-664d-49dc-988c-0a39cde6b968",
   "metadata": {},
   "source": [
    "# How do we compute the evidence in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026820cb-c1b7-4f04-b5cb-1a25e0beb5eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## _Intermezzo: Laplace method_\n",
    "\n",
    "We have an unnormalized probability density $P^*(x)$ with normalization constant $Z_P=\\int P^*(x) dx$ with a peak at $x_0$.\n",
    "\n",
    "We Taylor-expand the logarith of $P^*(x)$ around its peak:\n",
    "\n",
    "$$ \\log P^*(x) \\simeq \\log P^*(x_0) - \\frac{c}{2} \\left( x - x_0\\right)^2$$\n",
    "\n",
    "where\n",
    "\n",
    "$$c = - \\frac{d^2}{dx^2} \\log P^*(x)|_{x=x_0}$$\n",
    "\n",
    "We approximate $P^*(x)$ by an unnormalized Gaussian\n",
    "\n",
    "$$Q^*(x) = P^*(x_0) \\exp \\left[ -\\frac{c}{2} \\left( x - x_0\\right)^2 \\right]$$\n",
    "\n",
    "The results are easily generalized to the higher dimensional case using well-known properties of the Gaussian distribution:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
    "\n",
    "https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed793014-f42e-4e92-b55d-74abc8edddb7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The Laplace approximation to the evidence\n",
    "\n",
    "When there is a lot of data, the posterior becomes more peaked. We can thus use the Laplace method to approximate the evidence.\n",
    "\n",
    "$$p(D|H_i)=\\int dw p(D|w,H_i)p(w|H_i) \\approx p(D|w_\\text{MP},H_i)p(w_\\text{MP}|H_i)\\sqrt{\\frac{(2\\pi)^M}{\\det A}}$$\n",
    "\n",
    "$$\\log p(D|H_i) \\propto \\underbrace{\\log p(D|w_\\text{MP},H_i)}_{\\text{data likelihood}} - \\underbrace{\\left[\\frac{1}{2}\\log \\det \\left(\\frac{ A}{\\left(2\\pi\\right)^M}\\right)+\\log p(w_\\text{MP}|H_i)\\right]}_{\\text{complexity term}}$$\n",
    "\n",
    "with $A=-\\nabla\\nabla \\log p(w_\\mathrm{MP}|D,\\mathcal{H}_i)$.\n",
    "\n",
    "Since $\\det A =\\prod_{i=1}^M \\lambda_i$ we get $\\log \\det A \\propto M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1bd51-87f2-44f1-a587-4477c27f11e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## An example of model comparison: polynomial curve fitting\n",
    "\n",
    "Find the **best curve** explaining a given training set $(x_n,t_n), n=1,\\ldots,N$ using $M+1$ nonlinear functions $\\Phi_j(x)$ with parameters $w_j$:\n",
    "$$y(x,w)=\\sum_{j=0}^M w_j \\Phi_j(x)$$\n",
    "\n",
    "To do so, we seek to minimize the mean squared error\n",
    "$$E(w)=\\frac{1}{2}\\sum_{n=1}^N \\left( y(x_n,w)-t_n\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5f91a-db25-4129-99ea-44615528b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate problem\n",
    "num_x = 10\n",
    "eps = 0.2 # 0.\n",
    "seed = 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "# generate true underlying function\n",
    "x_to_plot = np.linspace(0., 1., 100)\n",
    "t_to_plot = np.sin(2*np.pi*x_to_plot)\n",
    "\n",
    "# generate learning problem\n",
    "x = np.linspace(0., 1., num_x) # set 10 points on the x axis between 0 and 2pi\n",
    "t_true = np.sin(2*np.pi*x) # generate true output\n",
    "t = t_true + np.random.randn(num_x) * eps # corrupt output with noise\n",
    "\n",
    "plt.plot(x_to_plot, t_to_plot, ':', label=\"sin(2$\\pi$x)\");\n",
    "plt.plot(x, t, 'o', label='t', color=\"black\");\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a82a0-ad10-4f3e-bb30-586135e4a3db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### How to choose the right number of parameters?\n",
    "\n",
    "We can take $\\Phi_j(x)=x^j$ so $y$ is an $M$th order polynomial:\n",
    "$$y(x,\\mathbf{w})=\\sum_{j=0}^M w_j x^j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32563c52-a066-48b6-ae53-42b5a37693f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit polys with varying degrees\n",
    "degs = [0, 1, 3, 9]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "for ideg, deg in enumerate(degs):\n",
    "    p = Polynomial(coef=np.random.randn(deg + 1))\n",
    "    p = p.fit(x,t,deg)\n",
    "    y_pred = p(x_to_plot)\n",
    "    plt.subplot(1, len(degs), ideg+1)\n",
    "    plt.plot(x_to_plot, t_to_plot, ':', alpha=0.5);\n",
    "    plt.plot(x, t, 'o', c=\"black\", label='t');\n",
    "    plt.plot(x_to_plot, y_pred, c=\"red\", label='y');\n",
    "    plt.legend();\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"t, y\")\n",
    "    plt.title(f'M = {deg}')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12556707-277a-415f-8db7-9c2b22f5f28c",
   "metadata": {},
   "source": [
    "### How to choose the right number of parameters?\n",
    "\n",
    "Split the data in a training set and a test set. And define\n",
    "\n",
    "$$\n",
    "E_\\text{train}(\\mathbf{w})=\\frac{1}{2}\\sum_{n\\in \\text{train}} \\left( y(x_n,\\mathbf{w})-t_n\\right)^2\\qquad\n",
    "E_\\text{test}(\\mathbf{w})=\\frac{1}{2}\\sum_{n\\in \\text{test}} \\left( y(x_n,\\mathbf{w})-t_n\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3916b95-8bc1-4b4d-b21d-c097d6085deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate problem\n",
    "num_train = 10\n",
    "num_test = 10\n",
    "eps = 0.1\n",
    "seed = 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "num_x = num_train + num_test\n",
    "\n",
    "# generate problem\n",
    "x = np.linspace(0., 1., num_x)\n",
    "t_true = np.sin(2*np.pi*x)\n",
    "t = t_true + np.random.randn(num_x) * eps\n",
    "\n",
    "# select train and test set randomly\n",
    "perm = np.arange(num_x)\n",
    "np.random.shuffle(perm)\n",
    "x_train, t_train = x[perm[:num_train]], t[perm[:num_train]]\n",
    "x_test, t_test = x[perm[num_train:]], t[perm[num_train:]]\n",
    "\n",
    "plt.plot(x_to_plot, t_to_plot, ':', label=\"sin(2$\\pi$x)\");\n",
    "plt.plot(x_train, t_train, 'o', label='train', color=\"black\");\n",
    "plt.plot(x_test, t_test, 'x', label='test', color=\"green\");\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3921ee4-07d4-4ba6-898b-836fbf879c24",
   "metadata": {},
   "source": [
    "## Complex models overfit\n",
    "\n",
    "Too simple (small $M$) $\\rightarrow$ poor fit\n",
    "\n",
    "Too complex (large $M$) $\\rightarrow$ overfitting (fits the noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35282421-e1be-4b01-ab3d-1a4a5ca678ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit polys with varying degrees\n",
    "degs = np.arange(9)\n",
    "\n",
    "errs_train, errs_test = np.zeros(len(degs)), np.zeros(len(degs))\n",
    "coeffs = []\n",
    "for ideg, deg in enumerate(degs):\n",
    "    p = Polynomial(coef=np.random.rand(deg + 1))\n",
    "    p = p.fit(x_train, t_train, deg)\n",
    "    coeffs.append(p.coef) # note: the fitting process shifts x -> [-1,1] so coef are not exacly w_j\n",
    "    y_train = p(x_train)\n",
    "    y_test = p(x_test)\n",
    "    errs_train[ideg] = 0.5 * ((y_train - t_train)**2).mean()\n",
    "    errs_test[ideg] = 0.5 * ((y_test - t_test)**2).mean()\n",
    "\n",
    "plt.plot(degs, errs_train, '.-', label=\"training\")\n",
    "plt.plot(degs, errs_test, '.-', label=\"test\")\n",
    "plt.xlabel('M')\n",
    "plt.ylabel('$E_{train}, E_{test}$')\n",
    "plt.legend();\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c852c3-b22d-430c-865a-ee1e61788092",
   "metadata": {},
   "source": [
    "## Complex models fit data using large weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703796d0-b040-4144-bd4b-7e783317cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the scale of the weights\n",
    "# WARNING: these are weights wrt a (possibly) shifted input variable x because of internal workings of numpy.Polynomial\n",
    "for ic, coef in enumerate(coeffs):\n",
    "    plt.plot(np.abs(coef), '.-', label=f'M = {degs[ic]}')\n",
    "plt.legend();\n",
    "plt.xlabel('M')\n",
    "plt.ylabel('abs of poly coeffs');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cf7eb-9ca8-45cc-a357-17f83acd25b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bayesian model comparison for regression\n",
    "\n",
    "<center><img src=\"figs/model_comparison_bayesian_regression.png\" width=400></center>\n",
    "\n",
    "Label the different models $H_i=M$ and assume some observation noise with (known) variance $\\sigma^2$. The probability of observing $t$ given $x$ reads:\n",
    "\n",
    "$$p(t|x,w,M) = \\mathcal{N}(t|y(x,w),\\beta) =\\sqrt{\\frac{\\beta}{2\\pi}}\\exp\\left(-\\frac{\\beta}{2}\\left(t-y(x,w)\\right)^2\\right)$$\n",
    "with $y(x,w)=\\sum_{i=0}^M w_i \\Phi_i(x)$. $\\beta = 1/\\sigma^2$ is called _precision_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b4caef-7ecb-4380-980c-c7124d541c0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bayesian model comparison for regression\n",
    "\n",
    "Prior over weights ($\\alpha$ is a parameter):\n",
    "$$p(w|M,\\alpha)=\\left(\\frac{\\alpha}{2\\pi}\\right)^{\\left(M+1\\right)/2} \\exp\\left(-\\frac{\\alpha}{2}\\sum_{i=0}^M w_i^2\\right)$$\n",
    "Likelihood:\n",
    "$$p(D|w,M,\\beta) = \\prod_{n=1}^N p(t_n|x_n,w,M) =\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\exp\\left(-\\frac{\\beta}{2}\\sum_{n=1}^N\\left(t_n-y(x_n,w)\\right)^2\\right)$$\n",
    "The evidence is (we assume $\\alpha,\\beta$ given):\n",
    "$$p(D|M,\\alpha,\\beta)=\\int dw p(D|w,M,\\beta)p(w|M,\\alpha)$$\n",
    "$$=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{\\left(M+1\\right)/2} \\int dw \\exp\\left(-\\frac{\\beta}{2}\\sum_{n=1}^N\\left(t_n-y(x_n,w)\\right)^2-\\frac{\\alpha}{2}\\sum_{i=0}^M w_i^2\\right)$$\n",
    "Since $y$ is linear in $w_i$, the Gaussian integral can be computed exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07a871-bde3-47e8-b73b-3aea4e4ae386",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bayesian model comparison for regression\n",
    "\n",
    "Denote $f(w)=\\frac{\\beta}{2}\\sum_{n=1}^N\\left(t_n-y(x_n,w)\\right)^2+\\frac{\\alpha}{2}\\sum_{i=0}^M w_i^2$. Then\n",
    "$$p(D|M,\\alpha,\\beta)=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{\\left(M+1\\right)/2} \\int dw e^{-f(w)}$$\n",
    "\n",
    "Since $f$ is a quadratic form, it agrees with its Taylor expansion to second order around any point (Laplace approximation is exact for a Gaussian).\n",
    "\n",
    "We can thus find minimize $f$, yielding the value $w_\\text{MP}$ (most probable $w$), and write:\n",
    "$$f(w)=f(w_\\text{MP}) +\\frac{1}{2}(w-w_\\text{MP})^T A (w-w_\\text{MP})\\qquad A_{ij} =\\alpha \\delta_{ij} + \\beta \\sum_{n=1}^N \\Phi_i(x_n)\\Phi_j(x_n)$$\n",
    "We thus get a Gaussian integral\n",
    "$$p(D|M,\\alpha,\\beta)=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{\\left(M+1\\right)/2}\\exp\\left(-f(w_\\text{MP})\\right)\\int dw \\exp\\left(-\\frac{1}{2}(w-w_\\text{MP})^T A (w-w_\\text{MP})\\right)$$\n",
    "$$=p(D|w_\\text{MP},M,\\beta)p(w_\\text{MP}|M,\\alpha)\\sqrt{\\frac{(2\\pi)^{M+1}}{\\det A}}$$\n",
    "since\n",
    "$$\n",
    "\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{\\left(M+1\\right)/2}\\exp\\left(-f\\left(w_{\\text{MP}}\\right)\\right)=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}e^{-\\frac{\\beta}{2}\\sum_{n=1}^{N}\\left(t_{n}-y\\left(x_{n},w\\right)\\right)^{2}}\\left(\\frac{\\alpha}{2\\pi}\\right)^{\\left(M+1\\right)/2}e^{-\\frac{\\alpha}{2}\\sum_{i=0}^{M}w_{i}^{2}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0262040-f092-4d1a-9984-bcae9773e8a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bayesian model comparison for regression\n",
    "\n",
    "The final step is to look at the evidence (or better the log evidence)\n",
    "$$\\log p(D|M,\\alpha,\\beta)= \\frac{M+1}{2}\\log\\alpha+\\frac{N}{2}\\log\\beta-f\\left(w_{\\text{MP}}\\right)-\\frac{1}{2}\\log\\det A-\\frac{N}{2}\\log2\\pi$$\n",
    "versus $M$ (for fixed $\\alpha, \\beta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024d282-2f5b-4013-a08c-483792709484",
   "metadata": {},
   "source": [
    "# <center>Assignments</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb192c77-7c68-4fd5-b8ce-1378b99f8594",
   "metadata": {},
   "source": [
    "#### Ex 2.3\n",
    "\n",
    "A photon counter is pointed at a remote star for one minute, in order\n",
    "to infer the rate of photons arriving at the counter per minute, $\\lambda$.\n",
    "Assuming the number of photons collected $r$ has a Poisson distribution\n",
    "with mean $\\lambda$:\n",
    "$p(r|\\lambda) = \\frac{e^{-\\lambda}\\lambda^r}{r\\!}$\n",
    "and assuming an improper prior $p(\\lambda) = \\frac{1}{\\lambda}$:\n",
    "\n",
    "  * compute the Laplace approximations $p_1(\\lambda)$ to\n",
    "    the posterior distribution.\n",
    "  * Consider the transformation to new coordinate $y=\\log\\lambda$, show\n",
    "    that the prior distribution over $y$ transforms to $p(y)=1$.\n",
    "  * Transform the posterior distribution to the new variable $y$ and\n",
    "    compute the Laplace approximation $p_2(y)$.\n",
    "  * Transform $p_2(y)$ back to $p_2(\\lambda)$.\n",
    "    Plot the posterior $p(\\lambda|r)$ and its two approximations\n",
    "    $p_1(\\lambda)$ and $p_2(\\lambda)$ for\n",
    "    $r=2$ and $r=10$ versus $\\lambda$. Which one is best and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9ef6e-5f89-4a30-841a-9dc472d1e2b2",
   "metadata": {},
   "source": [
    "#### Ex 2.4\n",
    "\n",
    "Data points $\\mathcal{D}=(x_i,t_i)$, $i=1,...,N$\n",
    "are believed to come from a straight line with noise. The experimenter\n",
    "chooses $x_i$ and assumes that $t_i$ is Gaussian distributed about\n",
    "$w_0+w_1x_i$\n",
    "with variance $\\sigma^2$. Model $\\mathcal{H}_1$ assumes the\n",
    "straight line to be horizontal ($w_1=0$), whereas model $\\mathcal{H}_2$\n",
    "assumes a normal prior on $w_1$, i.e. $w_1 \\sim \\mathcal{N}(0,1)$.\n",
    "Both models assign a normal prior to $w_0$. Given the data, what\n",
    "is the evidence for each model? Assume that data have been centered, namely $\\sum_{i=1}^N x_i=0$. Follow these steps.\n",
    "\n",
    "  * Write and expression $p(\\mathcal{D}|w_0,w_1,\\mathcal{H}_i)$.\n",
    "    Note that it is a product of functions of $w_0$ and $w_1$.\n",
    "  * Compute the odds ratio $\\frac{p(\\mathcal{D}|\\mathcal{H}_1)}{p(\\mathcal{D}|\\mathcal{H}_2)}$.\n",
    "    Show the result as a function of the input variance $s^2=\\frac{1}{N}\\sum_ix_i^2$,\n",
    "    the input-output correlation $c=\\frac{1}{N}\\sum_ix_it_i$. Hint:\n",
    "    make use of the factorization of the integrals over $w_0$ and $w_1$.\n",
    "  * Show that, in the limit of large $N$, the simpler model $\\mathcal{H}_1$\n",
    "    is preferred when $c^2 \\lesssim \\frac{\\log N}{N}$ and the complex\n",
    "    model is preferred otherwise. Hint: use the simplifying assumption\n",
    "    $\\sigma^2=\\langle x_i^2 \\rangle =1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2fa26-663d-46b0-a9c7-0ed10416dc29",
   "metadata": {},
   "source": [
    "#### Ex 2.5\n",
    "\n",
    "A $k$-sided die is rolled $n$ times and the outcomes are $\\mathcal{D}=(n_1,...,n_k)$,\n",
    "where $n_i$ is the number of times the die lands with side $i$\n",
    "up, $i=1,...,k$ and $\\sum_i^kn_i=n$.\n",
    "\n",
    "  * Compute the probability of the observed data assuming that the die\n",
    "    is perfectly fair ($\\mathcal{H}_0$).\n",
    "  * Compute the probability of the data under the alternative hypothesis\n",
    "    $\\mathcal{H}_1$ that the die has an unknown probability $p_i$\n",
    "    for outcome $i$ and that $p_i$ is uniformly distributed. Use the\n",
    "    fact that the probability density function of the Dirichlet distribution\n",
    "    with parameters $\\boldsymbol{\\alpha}$ is $p(\\boldsymbol{p}|\\boldsymbol{\\alpha})=\\frac{1}{B(\\boldsymbol{\\alpha})}\\prod_{i=1}^kp_i^{\\alpha_i-1}$.\n",
    "    This pdf lives on the simples $S_k$ defined by $0\\leq p_i \\leq 1$\n",
    "    and $\\sum_ip_i=1$. The normalization is given by the multivariate\n",
    "    Beta function\n",
    "    $B(\\boldsymbol{\\alpha}) = \\int_{S_k}d\\boldsymbol{p}\\prod_{i=1}^kp_i^{\\alpha_i-1} = \\frac{\\prod_{i=1}^k\\Gamma(\\alpha_i)}{\\Gamma(\\sum_i^k\\alpha_i)}$\n",
    "  * Consider $k=6$, $n=30$ and $\\boldsymbol{n}=(3,3,2,2,9,11)$.\n",
    "    Compute the posterior of the models assuming equal priors and show\n",
    "    that $\\mathcal{H}_1$ is more likely. Instead, when $n_i=5$ show\n",
    "    that $\\mathcal{H}_0$ is more likely.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
