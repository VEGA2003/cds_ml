{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee71b31a-086e-456f-bb77-545b4d04e3ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import color_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a441c4-7d71-4c9b-b6cc-fda0c282eb65",
   "metadata": {},
   "source": [
    "## Lecture 4: Learning in neural netwoks\n",
    "\n",
    "* **4.2 Feedforward networks**\n",
    "    * Multi-layered perceptrons, error backpropagation\n",
    "    * Regularization\n",
    "    * MLPs are universal approximators\n",
    "\n",
    "_Recommended readings_:\n",
    "* A classic introduction to early neural networks is contained in the book series [Parallel Distributed Processing](https://direct.mit.edu/books/monograph/4424/Parallel-Distributed-Processing-Volume)\n",
    "* The wiki article on the [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) is a good starting point for reading about recent developments after the [pioneering paper](https://link.springer.com/article/10.1007/BF02551274) by Cybenko."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d747b7-4250-40b1-ad29-6d651ca75137",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Multi-layered neural networks\n",
    "\n",
    "A Multilayer Perceptron (MLP) employs Fully Connected (FC) layers followed by nonlinearities $h$. An example of a two-layer (one-hidden layer) network with $M$ hidden neurons is the following:\n",
    "$$y_k(x,w)=h_2\\left(w_{k0}^{(2)}+\\sum_{j=1}^M w^{(2)}_{kj} h_1\\left(w_{j0}^{(1)}+\\sum_{i=1}^N w^{(1)}_{ji}x_i\\right)\\right)$$\n",
    "$h_{1,2}$ are scalar functions such as $x, \\sigma(x)$ or $\\text{ReLu}(x)$ [recall that $\\text{ReLu}(x) =x$ when $x>0$ and $\\text{ReLu}(x) =0$ otherwise].\n",
    "\n",
    "Usually one takes the activation function of the last hidden layer as the identity function and includes a softmax operation on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ec375-6f5f-4e19-b980-56d69e736279",
   "metadata": {},
   "source": [
    "## Cybenko Theorem (1989)\n",
    "\n",
    "Let $\\sigma$ be a sigmoidal function, i.e. $\\lim_{x\\to+\\infty}\\sigma\\left(x\\right)=1$ and $\\lim_{x\\to-\\infty}\\sigma\\left(x\\right)=0$. Then finite sums of the form\n",
    "$$g\\left(x\\right)=\\sum_{k=1}^{K}w_{k}^{\\left(2\\right)}\\sigma\\left(w_{k}^{\\left(1\\right)}\\cdot x+b_{k}^{\\left(1\\right)}\\right)$$\n",
    "are dense with respect to the supremum norm in the space $C\\left(I_{N}\\right)$ of continuous functions in the $N$-dimensional unit cube $\\left[0,1\\right]^{N}$ , i.e. given any $f\\in C\\left(I_{N}\\right)$ and any $\\epsilon>0$ there exists a $g$ such that\n",
    "$$|g\\left(x\\right)-f\\left(x\\right)|<\\epsilon$$\n",
    "for all $f\\in I_{N}$. Recall that $\\|f\\|_\\infty=\\sup_{I_N} \\{ \\left|f(x)\\right|, x \\in I_N\\}$.\n",
    "\n",
    "#### BUT\n",
    "\n",
    "A one-hidden-layer network will typically need an **exponentially large** $K$ to approximate a function with an $N$-dimensional input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb414488-3479-41b0-9d97-313c3e7e7233",
   "metadata": {},
   "source": [
    "## Universal approximation theorems\n",
    "\n",
    "More general existence theorems exist for\n",
    "* non-polynomial activation functions;\n",
    "* MLPs with more than one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fcd3f0-b947-4ccb-aba6-25a226baa79d",
   "metadata": {},
   "source": [
    "### Example with a one-hidden layer network\n",
    "\n",
    "Consider $2^N$ binary patterns $x_i^\\mu= \\pm 1$ in $N$ dimensions and two classes $x^\\mu \\rightarrow t^\\mu=\\pm 1$.\n",
    "\n",
    "Use $2^N$ hidden units, labeled $k=0,\\ldots,2^N-1$ and make sure that each hidden unit only responds to a one of the possible $2^N$ patterns.\n",
    "\n",
    "To this aim, set:\n",
    "* $w^{(1)}_{ki}=b$ if $i$th digit in binary representation of $k$ is 1\n",
    "* $w^{(1)}_{ki}=-b$ otherwise\n",
    "\n",
    "and use a threshold of $(N-1)b$ at each hidden units, with a Heaviside activation function for the hidden layer and a sign activation function at the output:\n",
    "$$z_k=\\Theta\\left[\\sum_i w^{(1)}_{ki}x_i-(N-1)b\\right]$$\n",
    "$$y=\\text{sign}\\left[\\sum_{k=0}^{K-1} w^{(2)}_k z_k\\right]$$\n",
    "\n",
    "Here's an example with $N=2$-dimensional inputs and $2^N=4$ hidden units:\n",
    "\n",
    "| $k$ | binary repr. | $w^{(1)}_{k1}$ | $w^{(1)}_{k2}$ |\n",
    "|---|---|---|---|\n",
    "| 0 | 00 | -b | -b |\n",
    "| 1 | 01 | -b | b |\n",
    "| 2 | 10 | b | -b |\n",
    "| 3 | 11 | b | b |\n",
    "\n",
    "Consider the pre-activation in the presence of all possible $P=2^N$ inputs $x$:\n",
    "| $x_1$ | $x_2$ | $\\sum_i w^{(1)}_{0i}x_i$ | $w^{(1)}_{1i}x_i$ | $w^{(1)}_{2i}x_i$ | $w^{(1)}_{3i}x_i$ |\n",
    "|---|---|---|---|---|---|\n",
    "| -1 | -1 | **2b** | 0 | 0 | -2b |\n",
    "| -1 | 1 | 0 | **2b** | -2b | 0 |\n",
    "| 1 | -1 | 0 | -2b | **2b** | 0 |\n",
    "| 1 | 1 | -2b | 0 | 0 | **2b** |\n",
    "\n",
    "In the $2^N$ dimensional $z$ space the problem is linearly separable in a trivial fashion:\n",
    "\n",
    "| $x_1$ | $x_2$ | $z_0$ | $z_1$ | $z_2$ | $z_3$ | $y$ |\n",
    "|---|---|---|---|---|---|---|\n",
    "| -1 | -1 | 1 | 0 | 0 | 0 | $\\text{sign}[w^{(2)}_0]$ |\n",
    "| -1 | 1 | 0 | 1 | 0 | 0 | $\\text{sign}[w^{(2)}_1]$ |\n",
    "| 1 | -1 | 0 | 0 | 1 | 0 | $\\text{sign}[w^{(2)}_2]$ |\n",
    "| 1 | 1 | 0 | 0 | 0 | 1 | $\\text{sign}[w^{(2)}_3]$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c92fb-002e-43c7-a009-c5f8c8330dc5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## MLP for regression\n",
    "\n",
    "Let us use a two-layer NN with 3 $\\tanh$ hidden units and linear output to approximate functions. We take equally spaced points in $x\\in [-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f3ce90-6fa0-477f-bc47-e638f7fec172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my own network\n",
    "def my_own_net(mlp, x, act_name = \"tanh\"):\n",
    "    if act_name == \"tanh\":\n",
    "        act = np.tanh\n",
    "    elif act_name == \"relu\":\n",
    "        act = lambda x: (x>0) * x\n",
    "    else:\n",
    "        raise ValueError(\"Unknwon activation function\")\n",
    "    W, v = mlp.coefs_\n",
    "    b_W, b_v = mlp.intercepts_\n",
    "    a_pred = act(x @ W + b_W)\n",
    "    y_pred = a_pred @ v + b_v\n",
    "    return a_pred, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f0fe2-ea1a-4d46-9c46-dfa968ca68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set input\n",
    "x = np.linspace(-1, +1, 100)\n",
    "\n",
    "# uncomment to select output function\n",
    "# y = x**2\n",
    "# y = np.sin(x)\n",
    "# y = np.abs(x)\n",
    "y = np.sign(x)\n",
    "\n",
    "# regularization controls the final scale of learned weights and biases: look at the different effect of 0.1 and 0.0\n",
    "regularization = 0.0001\n",
    "\n",
    "# train networks\n",
    "mlp = MLPRegressor(hidden_layer_sizes=3, alpha=regularization, activation=\"tanh\", solver=\"lbfgs\", momentum=0.,\n",
    "                   max_iter=100000, learning_rate_init=0.01, tol=1e-5, random_state=1, verbose=False)\n",
    "# compute predicted outputs\n",
    "mlp.fit(x[:,None], y)\n",
    "y_pred = mlp.predict(x[:,None])\n",
    "\n",
    "a_pred, y_pred_check = my_own_net(mlp, x[:,None], act_name = \"tanh\")\n",
    "\n",
    "# plot network output\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(x, y, label=\"true output\");\n",
    "plt.plot(x, y_pred, '.', label=\"predicted output\");\n",
    "plt.plot(x, y_pred_check[:,0], '.', alpha=0.2, label=\"just a check\"); # just a check\n",
    "lines = plt.plot(x, a_pred, '--', alpha=0.3, label=\"hidden activations\"); # visualize internal activities\n",
    "plt.setp(lines[1:], label=\"_\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(mlp.coefs_[0][0], '.-', label=\"W\");\n",
    "plt.plot(mlp.coefs_[1][:,0], '.-', label=\"v\");\n",
    "plt.legend();\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "003cbdb8-01ca-4c04-a9d4-209e49974345",
   "metadata": {},
   "source": [
    "## MLP for classification\n",
    "\n",
    "Adapted from sklearn tutorial [Classifier comparison](https://scikit-learn.org/1.5/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c955fc5-5a56-45c1-a2b6-0276c4714e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "np.random.seed(1)\n",
    "num_hidden = 30\n",
    "n_samples = 500\n",
    "noise = 0.2\n",
    "\n",
    "# get dataset\n",
    "X, y = make_circles(n_samples, noise=noise, factor=0.5, random_state=1)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "# split dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# train classifier and get boundaries\n",
    "mlp = MLPClassifier(activation=\"relu\", hidden_layer_sizes=num_hidden, alpha=1, max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "score = mlp.score(X_test, y_test)\n",
    "\n",
    "figure = plt.figure(figsize=(12, 4))\n",
    "# just plot the dataset first\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "# Plot the training and test points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.5, edgecolors=\"k\");\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "# Plot the training and test points\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "DecisionBoundaryDisplay.from_estimator(mlp, X, ax=ax, cmap=cm, alpha=0.8, eps=0.5)\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, edgecolors=\"k\",alpha=0.6);\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36434fcf-7759-452a-aca3-a12ecd4f31e3",
   "metadata": {},
   "source": [
    "#### Distribution of pre-outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366db674-c468-400b-b79f-ae520fbdd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels from trained mlp\n",
    "label_pred = mlp.predict(X_train)\n",
    "\n",
    "# extract parameters and compute internal activities\n",
    "_, y_pred_check = my_own_net(mlp, X_train, act_name=\"relu\")\n",
    "\n",
    "# uncomment to check outputs with your own computation\n",
    "proba_pred = mlp.predict_proba(X_train)\n",
    "# plt.plot(proba_pred[:,1], proba_pred[:,1], ':')\n",
    "# plt.plot(proba_pred[:,1], np.exp(y_pred_check)/(1.+np.exp(y_pred_check)), '.');\n",
    "# plt.xlabel('proba pred')\n",
    "# plt.ylabel('proba pred check');\n",
    "\n",
    "# visualize last layer activation per label\n",
    "plt.hist(y_pred_check[label_pred==0], bins=\"auto\", label=\"label=0\");\n",
    "plt.hist(y_pred_check[label_pred==1], bins=\"auto\", label=\"label=1\");\n",
    "plt.xlabel(\"pre-output\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e6c6f-0e0e-4dbc-8306-16b35a8da559",
   "metadata": {},
   "source": [
    "#### Have a look at internal representations of each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcc9d4-34c8-488b-a867-3d9513efefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract internal representation in a 2d grid\n",
    "num_a = num_hidden\n",
    "X, Y = np.meshgrid(np.linspace(x_min, x_max, 20), np.linspace(y_min, y_max, 20))\n",
    "x_stacked = np.dstack([X,Y])\n",
    "y_stacked = np.zeros((20, 20, num_a))\n",
    "for irow, row in enumerate(x_stacked):\n",
    "    rowy, _ = my_own_net(mlp, row, act_name=\"relu\")\n",
    "    y_stacked[irow] = rowy[:,:num_a]\n",
    "\n",
    "nrows, ncols = num_a//5, 5\n",
    "fig, axes = plt.subplots(nrows, 5)\n",
    "fig.set_size_inches(8,nrows)\n",
    "extent = [x_min, x_max, y_min, y_max]\n",
    "for count in range(num_a):\n",
    "    i, j = count // 5, count % 5\n",
    "    axes[i,j].imshow(y_stacked[:,:,count], cmap=cm, extent=extent)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aad40b-e7a0-415f-992b-bd244efb4cfe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Local minima\n",
    "\n",
    "Error is affected by local minima.\n",
    "\n",
    "Part of the cause of local minima is the **saturation of the sigmoid functions** $$\\sigma\\left(\\sum_j w_{ij} x_j\\right)$$ When $w_{ij}$ becomes large, any change in its value hardly affects the output, implying $\\nabla_{ij}E=0$.\n",
    "\n",
    "You can partly prevent this from happening by:\n",
    "* Choosing **tanh** instead of sigmoid transfer functions and scaling inputs and outputs to have mean zero and standard deviation one.\n",
    "* Choosing **ReLU** activation functions.\n",
    "* **Proper initialization** of $w_{ij}$ with mean zero and a standard deviation of order $1/\\sqrt{n_1}$, where $n_1$ is the number of inputs to neuron $i$.\n",
    "* Adding a **regularizer** such as $\\sum_{i} w_i^2$ to the cost function to keep weights small.\n",
    "* Using techniques like dropouts, layer normalization, adding noise, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa43c7-4c0f-4f5d-8c22-5561762e1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a networks starting from different initial conditions\n",
    "num_hidden = 3\n",
    "num_seeds = 100\n",
    "print_every = 20\n",
    "noise = 0.1\n",
    "regularization = 0.01\n",
    "\n",
    "# generate input and output\n",
    "np.random.seed(1)\n",
    "X_train = np.linspace(-np.pi, np.pi, 10)\n",
    "y_train = np.sin(X_train) + np.random.randn(len(X_train)) * noise\n",
    "X_test = np.linspace(-np.pi, np.pi, 100)\n",
    "y_test = np.sin(X_test)\n",
    "\n",
    "plt.plot(X_test, y_test, ':')\n",
    "plt.plot(X_train, y_train, 'o');\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53558fbd-15e8-4a57-bac9-ca1dba549bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = np.zeros((num_seeds, len(y_test)))\n",
    "errs = np.zeros(num_seeds)\n",
    "\n",
    "for seed in range(1, num_seeds+1):\n",
    "    if seed % print_every == 0:\n",
    "        print(\"training with seed\", seed)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=num_hidden, alpha=regularization, activation=\"tanh\", solver=\"lbfgs\", momentum=0.,\n",
    "                       max_iter=10000, learning_rate_init=0.01, tol=1e-5, random_state=seed, verbose=False)\n",
    "    mlp.fit(X_train[:,None], y_train)\n",
    "    y_test_pred = mlp.predict(X_test[:,None])\n",
    "    y_test_preds[seed-1] = y_test_pred\n",
    "    errs[seed-1] = ((y_test - y_test_pred)**2).mean()\n",
    "print(\"\\n\")\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(X_test, y_test, '.-', c=\"black\", ms=3, label=\"y test\")\n",
    "lines = plt.plot(X_test, y_test_preds.T, '.-', alpha=0.2, color='gray', ms=3, label=\"y test pred\");\n",
    "plt.setp(lines[1:], label=\"_\")\n",
    "plt.legend();\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.subplot(122)\n",
    "plt.hist(errs);\n",
    "plt.xlabel('err')\n",
    "plt.ylabel('# err');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc03ae-039c-40b8-bff9-bf3e1dbd95c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training an MLP with gradient descent: backpropagation of error #1\n",
    "\n",
    "We will use the Mean Squared Error (MSE) loss. The error is the sum of error per pattern using :\n",
    "$$E(w) = \\sum_\\mu E_\\mu(w)\\qquad E_\\mu(w)=\\frac{1}{2}\\sum_k\\left(y_k(x^\\mu,w)-t_k^\\mu\\right)^2$$\n",
    "\n",
    "$$y_k=h\\left(\\underbrace{\\sum_j w_{kj}^{(2)} z_j^{(1)}}_{a_k^{(2)}}\\right)\\qquad z_j^{(1)}=h\\left(\\sum_i w_{ji}^{(1)} x_i\\right)$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w^{(2)}_{kj}}\\frac{1}{2}\\sum_{k'}(y_{k'}-t_{k'})^2=(y_k-t_k) \\frac{\\partial y_k}{\\partial w^{(2)}_{kj}}={\\color{red} \\underbrace{ (y_k-t_k) h'(a_k^{(2)}) }_{\\delta_k^{(2)}}}z_j^{(1)}={\\color{red} \\delta^{(2)}_k} z_j^{(1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f44c7e-50ee-4898-a707-b2292e525b41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training an MLP with gradient descent: backpropagation of error #2\n",
    "\n",
    "Similarly:\n",
    "$$y_k =h\\left(\\sum_j w_{kj}^{(2)} h\\left(\\sum_i w_{ji}^{(1)} z_i^{(0)}\\right)\\right)$$$$\\frac{\\partial}{\\partial w^{(1)}_{ji}}\\frac{1}{2}\\sum_{k}(y_{k}-t_k)^2=\\sum_{k=1}^K (y_k-t_k) \\frac{\\partial y_k}{\\partial w^{(1)}_{ji}}$$$$=\\underbrace{\\sum_{k=1}^K \\underbrace{(y_k-t_k) h'(a^{(2)}_k)}_{\\delta_k^{(2)}}w_{kj}^{(2)}h'(a_j^{(1)})}_{\\delta_j^{(1)}}z_i^{(0)}= \\textcolor{red}{\\delta_j^{(1)}} z_i^{(0)}$$with$$\\delta_j^{(1)}=h'(a_j^{(1)})\\sum_{k=1}^K \\delta_k^{(2)} w^{(2)}_{kj}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d1c54-12e5-44d1-aff4-af2d4106e2fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training an MLP with gradient descent: backpropagation of error #3\n",
    "\n",
    "Backpropagation extends to $T$ layers. For each pattern $\\mu$:\n",
    "1.  Compute forward activities $a_j^{\\mu,(t)}=\\sum_i w_{ji}^{(t)}z_i^{\\mu,(t-1)}$ and $z_j^{\\mu,(t)}=h\\left(a_j^{\\mu,(t)}\\right), t=1,\\ldots, T$.\n",
    "\n",
    "    With $z_i^{\\mu,(0)}=x_i^\\mu$.\n",
    "    \n",
    "2.  Compute the errors $\\delta_j^{\\mu,(t-1)}=h'(a_j^{\\mu,(t-1)})\\sum_k w_{kj}^{(t)} \\delta_k^{\\mu,(t)}, t=T,\\ldots,2$.\n",
    "\n",
    "    With $\\delta_k^{\\mu,(T)}=(y^\\mu_k-t^\\mu_k)h'(a_k^{\\mu,(T)})$.\n",
    "\n",
    "3.  $\\frac{\\partial E_\\mu}{\\partial w^{(t)}_{ji}} = \\delta_j^{\\mu,(t)} z^{\\mu,(t-1)}_i$\n",
    "\n",
    "The gradient is $\\frac{\\partial E}{ \\partial w^{(t)}_{ji}}=\\sum_\\mu \\frac{\\partial E_\\mu}{\\partial w^{(t)}_{ji} }$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3fae2fa-5da4-47d6-99c4-1b300801555e",
   "metadata": {},
   "source": [
    "# <center>Assignments</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf9bb3-3056-4d5f-a8e1-be7cf170bce7",
   "metadata": {},
   "source": [
    "#### Ex 4.4\n",
    "\n",
    "Write your own version of a one-hidden layer neural network trained with MSE as in the section **Local minima** and train it on the same dataset using gradient descent. Experiment with both $\\tanh$ and ReLu activation functions.\n",
    "\n",
    "Report the training and test error over training and visualize the final regression performance for three values of `num_hidden`: 3, 10, 50."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
